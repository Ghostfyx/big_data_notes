# 第三章 Hadoop分布式文件系统

当数据集大小超过一台独立的物理机器的存储能力时，就有必要对它进行分区（Partition）并存储在若干台单独的机器上。管理网络中跨多台计算机存储的文件系统称为分布式文件系统（distributed filesystem）。该系统架构于网络之上，因此会引入网络编程的复杂性，因此分布式文件系统比普通的磁盘文件系统更为复杂。

Hadoop的分布式文件系统称为Hadoop Distributed FileSystem，简称HDFS。

## 3.1 HDFS的设计

HDFS以流式数据访问模式来存储超大规模文件，运行与商用硬件集群上。

- **超大文件    **指的是以GB，TB，甚至PB级别的数据规模
- **流式数据访问     ** HDFS的设计思路是基于一次写入，多次读取的最高效模式。数据集通常是由数据源生成或从数据源复制迁移而来，接着长时间在数据集上进行各种分析，每次分析都会涉及大部分数据，甚至全部数据，因此读取整个数据集的时间延迟比读取第一条记录的时间延迟更为重要（我的理解是HDFS考虑整体数据加载的延迟，而业务系统关心单条数据加载的效率）。
- **商用硬件    **Hadoop并不需要运行在昂贵且高可靠的硬件上，HDFS具有High Avaliable，遇到故障时，能够继续运行且不会被用户察觉。
- **低时间延迟访问数据     **要求低时间延迟数据访问的应用，不适合在HDFS上运行。HDFS是为了高数据吞吐，高时间延迟应用优化的。目前，对于低时间延迟的应用可以使用**HBase**。
- **大量的小文件    ** 由于nameNode将文件系统元数据存储在内存，因此HDFS文件存储能力受限于nameNode的内存容量。根据经验，每个文件，目录和数据块的存储信息大约占150字节。假设存储100万个文件，每个文件占一个数据块，那么至少需要300MB内存，存储数亿文件就超出了当前硬件的能力。
- **多用户写入，任意修改文件    **HDFS中的文件可能只有一个Writer，而且写操作总是将数据添加在文件末尾。**HDFS不支持具有多个写入着的写入操作，也不支持文件的任意位置修改操作。**

## 3.2 HDFS的概念

### 3.2.1 数据块Block

每个磁盘都会有默认的数据块大小，这是磁盘进行数据读/写的最小单位，构建与单个磁盘之上的文件系统通过磁盘块来管理该文件系统中的块，文件系统的大小可以是块的整数倍。磁盘块大小一般是512字节。

HDFS也有**Block块**的概念，默认128MB（老版本是64MB），与单一文件磁盘相似，HDFS上的文件被划为多个块（chunk）作为独立存储单元，但与其他文件系统不同的是，HDFS中的一个块大小的文件不会占据整个块的空间。

​												**Tips  HDFS中的块为什么这么大**

HDFS中的文件块比磁盘块大，其目的是为了最小化寻址开销，如果块设置的足够大，从磁盘传输文件的文件会明显大于大于定位这个块所需要的时间。因此，传输一个由多个块组成的HDFS文件时间取决去磁盘传输速率。这种做法改变了文件传输效率的瓶颈。

如果寻址时间是10ms，传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，需要将块的大小设置为100MB，随着磁盘驱动器传输速率的提升，块的大小会被设置成更大。但是不能设置过大，Hadoop MapReduce中的Map任务通常只处理一个块中的数据，因此如果任务太少，会影响作业的运行速度。

HDFS对文件块抽象的优点：

- 一个文件的大小可以大于Hadoop集群中任意一个磁盘的容量，同一文件的文件块并不需要存储在一个磁盘上，可以利用集群的任意磁盘进行存储。
- 简化了分布式文件系统的存储系统子设计，存储子系统控制单元为HDFS文件块，块的大小固定，计算单个磁盘能够存储多少个块很容易。
- 将文件权限信息，存储位置等元数据与文件实际数据块解耦，文件元数据与文件不必一起存储，可以使用其他系统管理元数据。
- HDFS文件块适用于数据备份，进而提供数据容错能力和提高可用性，HDFS高可用的原因之一。将每个块复制多个副本（默认3个）到集群中其他机器，可以确保在块、磁盘或机器发生故障后数据不会丢失。一个因损坏或机器故障而丢失的块，可以从其他副本存储地址复制到另一台机器，保证副本数量保持完整性。

与磁盘文件系统相似，Hadoop文件系统fsck指令可以显示块信息。

```sh
hadoop fsck / -files -blocks
```

### 3.2.2 namenode和datanode

HDFS集群有两类节点以管理者-工作者模式运行，即一个namenode（管理者）和多个datanode（工作者）。

- **namenode**管理文件系统的命名空间，维护文件系统树和树内所有的文件和目录。这些信息以两个文件形式永久保存在本地磁盘上：**命名空间镜像文件和编辑日志文件**。datanode也会记录每个文件中各个块所在数据节点信息，但它不会永久存储，这些信息会在系统启动时由数据节点重建。
- **客户端（Client）**代表用户通过namenode和datanode交互来访问整个文件系统。客户端提供一个类似于POSIX（可移植操作系统界面）的文件系统接口，因此用户无需知道namenode和datanode也可以操作文件系统。
- **datanode**是文件系统的工作节点，根据需要存储并检索数据块，受客户端与namenode调度，并且定期向namenode发送存储的块的列表。

没有namenode文件系统将无法使用，如果namenode服务的机器故障或namenode信息损坏，这个文件系统的文件将丢失（namenode存储元数据，我们不知道如何根据datanode的块重建文件）。对namenode块实现容错非常重要，Hadoop提供两种机制：

1. 第一种机制：**备份元数据持久状态的文件**。Hadoop可以通过配置使namenode在多个文件系统上保存元数据的持久状态。这些写入操作是同步的原子操作。一般配置是，将持久状态写入本地磁盘的同时，写入一个远程挂载的网络文件系统（NFS）。
2. 第二种机制：**运行备份namenode**。备份namenode不能被用作namenode，主要是定期通过编辑日志合并命名空间镜像文件，以防止编辑日志过大。备份namenode一般在另一台机器上，因为需要占用大量的CPU时间和磁盘执行合并操作。在主namenode出现故障时，启用备份namenode，但是编辑日志与命名空间文件同步总会有延迟，解决方法是：**把存储在NFS上的namenode元数据复制到备份namenode，并作为主namenode运行。**

### 3.2.3 联邦HDFS

namenode在内存中保存文件系统中每个文件和每个数据块的引用关系，对于保存大量文件的超大HDFS集群来说，内存成为限制集群横向扩展的瓶颈。Hadoop 2X版本引入联邦HDFS的概念，通过添加namenode实现扩展，每个namenode管理文件系统命名空间的一部分，例如：namenode A管理/user下所有文件的命名空间，namenode B管理/share下所有文件的命名空间。

每个namenode管理一个独立的命名空间卷（namespace volume）包括**命名空间的源数据和该命名中间下的所有文件的所有数据块的数据块池。**命名空间卷之间相互独立，两两不相互通信。数据块池不再切分，因此集群中的datanode需要注册到每个namenode。

### 3.2.4 HDFS高可用性

