# 第四章 键值操作

键值对RDD是Spark中许多操作需要的常见数据类型，键值对RDD通常用于聚合操作，我们一般要先通过初试ETL（抽取，转换，装载）操作将数据转换为键值对形式。

本章也会讨论用来让用户控制键值对 RDD 在各节点上分布情况的高级特性:分区。有时， 使用可控的分区方式把常被一起访问的数据放到同一个节点上，可以大大减少应用的通信 开销。这会带来明显的性能提升。我们会使用 PageRank 算法来演示分区的作用。为分布 式数据集选择正确的分区方式和为本地数据集选择合适的数据结构很相似——在这两种情 况下，数据的分布都会极其明显地影响程序的性能表现。 

## 4.1 动机

Spark为包含键值对类型的RDD提供了一些专有操作。这些RDD被称为pairRDD，pairRDD是很多程序的构成要素，因为它们提供了并行操作或跨节点重新进行数据分组的操作接口。例如：reduceByKey( )。

## 4.2 创建PairRDD

在Spark中有许多创建PairRDD的方式：1. 很多存储键值对的数据格式会 在读取时直接返回由其键值对数据组成的 pair RDD。2. 普通 RDD 转pair RDD，可以调用 map() 函数来实现，传递的函数需要返回键值对。

构建键值对 RDD 的方法在不同的语言中会有所不同。

- Python了让提取键之后的 数据能够在函数中使用，需要返回一个由二元组组成的 RDD 

	```python
	pairs = lines.map(lambda x: (x.split(" ")[0], x)) # (key, value)形式的数据
	```

- scala为了让提取键之后的数据能够在函数中使用，同样需要返回二元组 

	```scala
	val pairs = lines.map(x => (x.split(" ")(0), x))
	```

- Java 没有自带的二元组类型，因此 Spark 的 Java API 让用户使用 scala.Tuple2 类来创建二 元组。这个类很简单:Java 用户可以通过 new Tuple2(elem1, elem2) 来创建一个新的二元 组，并且可以通过 .__1() 和 ._2() 方法访问其中的元素。 Java 用户还需要调用专门的 Spark 函数来创建 pair RDD。例如，要使用 mapToPair() 函数 来代替基础版的 map() 函数， 

	```java
	PairFunction<String, String, String> keyData =
	       new PairFunction<String, String, String>() {
	       public Tuple2<String, String> call(String x) {
	         return new Tuple2(x.split(" ")[0], x);
	       }
	     };
	JavaPairRDD<String, String> pairs = lines.mapToPair(keyData);
	
	```

## 4.3 PairRDD 转换操作

Pair RDD可以使用标准RDD上所有可用操作，3.4节中的函数传递规则也同样适用，但是PairRDD包含二元组，因此需要传递的函数应该操作二元组而不是独立元素。表 4-1 和表 4-2 总结了对 pair RDD 的一些转化操作 。

​								表4-1:Pair RDD的转化操作(以键值对集合{(1, 2), (3, 4), (3, 6)}为例) 

| 函数名                                                       | 含义                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| reduceByKey(func)                                            | 合并具有相同key的二元组                                      |
| foldByKey(zeroValue: V, numPartitions: Int)                  | foldByKey操作作用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用与V，进行初始化V，在将映射函数应用于初始化后的V。 |
| groupByKey([*numTasks*])                                     | 对二元组按key分组                                            |
| combineBykey(createCombiner,mergeValue, mergeCombiners, partitioner) |                                                              |
| mapValues(func)                                              | 对 pair RDD 中的每个值应用 一个函数而不改变键                |
| flatMapValues(func)                                          | 对 pair RDD 中的每个值应用 一个返回迭代器的函数，然后 对返回的每个元素都生成一个 对应原键的键值对记录。通常 用于符号化 |
| keys()                                                       | 返回一个仅包含键的 RDD                                       |
| values()                                                     | 返回一个仅包含值的 RDD                                       |
| sortByKey()                                                  | 返回一个根据键排序的 RDD                                     |

​												表4-2:针对两个pair RDD的转化操作 

| 函数名                   | 含义                                                         |
| ------------------------ | ------------------------------------------------------------ |
| subtractByKey(otherRDD)  | 删掉 RDD 中键与 other RDD 中的键相同的元素                   |
| join(otherRDD)           | 对两个 RDD 进行内连接                                        |
| rightOuterJoin(otherRDD) | 对两个 RDD 进行连接操作，确保第一 个 RDD 的键必须存在(右外连接) |
| leftOuterJoin(otherRDD)  | 对两个 RDD 进行连接操作，确保第二 个 RDD 的键必须存在(左外连接) |
| cogroup(otherRDD)        | 将两个 RDD 中拥有相同键的数据分组到一起                      |

Pair RDD 也还是 RDD(元素为 Java 或 Scala 中的 Tuple2 对象或 Python 中的元组)，因此 同样支持 RDD 所支持的函数。例如，我们可以拿前一节中的 pair RDD，筛选掉长度超过 20 个字符的行：

```python
# 例 4-4:用 Python 对第二个元素进行筛选
result = pairs.filter(lambda keyValue: len(keyValue[1]) < 20)
```

```scala
// 例 4-5:用 Scala 对第二个元素进行筛选
pairs.filter{case (key, value) => value.length < 20}
```

```java
// 例 4-6:用 Java 对第二个元素进行筛选
Function<Tuple2<String, String>, Boolean> longWordFilter =
       new Function<Tuple2<String, String>, Boolean>() {
         public Boolean call(Tuple2<String, String> keyValue) {
           return (keyValue._2().length() < 20);
} };
JavaPairRDD<String, String> result = pairs.filter(longWordFilter);
```

下面具体讨论PairRDD的各种函数操作。

### 4.3.1 聚合操作

当数据集以键值对形式组织的时候，聚合具有相同key的元素进行统计是一种常用操作。上一章讲了基础RDD上的fold( )，combine( )，reduce( )等行动操作，PairRDD上也有相对应的键的转换操作。**注意：PairRDD上是转换操作，而不是行动操作**。

reduceByKey( )与reduce( )相类似：都接收一个函数，并使用该函数对值进行合并。reduceByKey( )会为数据集中的每一个键进行并行的归约操作（对应MapReduce计算框架的Reduce），每个归约操作会将键相同的值合并起来，最终返回一个由各键和对应键归约出来的结果值组成的新的 RDD。 

foldByKey( )与fold( )类似：都使用一个与RDD和合并函数中数据类型相同的零值作为初始值，与 fold() 一样，foldByKey() 操作所使用的合并函数对零值与另一 个元素进行合并，结果仍为该元素。 

combineByKey( )是最常用的基于键进行聚合的函数，大多数基于键聚合的函数都是用它 实现的。和 aggregate() 一样，combineByKey() 可以让用户返回与输入数据的类型不同的返回值。combineByKey会遍历分区的所有元素，如果这是一个新元素，combineByKey( )会使用createCombiner( )函数来创建那个键对应的累加器的初始值，注意：**这个过程会在每个分区中第一次出现各个键时发生，而不是在整个RDD中第一次出现一个键发生**。

如果分区中已经遇到过key，它会使用mergeValue( )方法将该键的累加器对应的当前值与这个新值进行合并。

由于每个分区都是独立并行处理的，因此对于同一个键可以有多个累加器，如果有两个或者更 多的分区都有对应同一个键的累加器，就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。

combineByKey( )有多个参数分别对应聚合操作的各个阶段，代码示例如下：

```python
# 在 Python 中使用 combineByKey() 求每个键对应的平均值
sumCount = nums.cobineByKey((lambda x : (x, 1)),
														(lambda x, y: (x[0]+y, x[1]+1)),
														(lambda x, y: (x[0]+y[0], x[1]+y[1]))
)
sumCount.map(lambda key, xy : (xy[0]/xy[1])).collectAsMap()
```

```scala
// 在 Scala 中使用 combineByKey() 求每个键对应的平均值
 val result = input.combineByKey(
       (v) => (v, 1),
       (acc: (Int, Int), v) => (acc._1 + v, acc._2 + 1),
       (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
       ).map{ case (key, value) => (key, value._1 / value._2.toFloat) }
       result.collectAsMap().map(println(_))
```

```java
// 在 Java 中使用 combineByKey() 求每个键对应的平均值
public static class AvgCount implements Serializable {
       public AvgCount(int total, int num) {   total_ = total;  num_ = num; }
       public int total_;
       public int num_;
       public float avg() {   returntotal_/(float)num_; }
}
Function<Integer, AvgCount> createAcc = new Function<Integer, AvgCount>() {
       public AvgCount call(Integer x) {
         return new AvgCount(x, 1);
       }
};
Function2<AvgCount, Integer, AvgCount> addAndCount =
       new Function2<AvgCount, Integer, AvgCount>() {
       public AvgCount call(AvgCount a, Integer x) {
         a.total_ += x;
         a.num_ += 1;
         return a;
} };
Function2<AvgCount, AvgCount, AvgCount> combine =
       new Function2<AvgCount, AvgCount, AvgCount>() {
       public AvgCount call(AvgCount a, AvgCount b) {
         a.total_ += b.total_;
         a.num_ += b.num_;
         return a;
} };
AvgCount initial = new AvgCount(0,0);
JavaPairRDD<String, AvgCount> avgCounts = nums.combineByKey(createAcc, addAndCount, combine);
Map<String, AvgCount> countMap = avgCounts.collectAsMap();
for (Entry<String, AvgCount> entry : countMap.entrySet()) {
      System.out.println(entry.getKey() + ":" + entry.getValue().avg());
}
```

![](./img/4-3.png)

有很多函数可以进行基于键的数据合并。它们中的大多数都是在 combineByKey() 的基础上实现的，为用户提供了更简单的接口。

**并行度调优**

每个RDD都有固定数目的分区，分区数决定了在RDD上执行操作时的并行度，在执行**聚合或者分组操作**的时候，可制定分区数目，Spark也始终尝试根据集群的大小推断出一个有意义的默认值。

```python
# 在 Python 中自定义 reduceByKey() 的并行度
data = [("a", 3), ("b", 4), ("a", 1)] 
sc.parallelize(data).reduceByKey(lambda x, y: x + y) # 默认并行度
sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10) #自定义并行度
```

```scala
// 在 Scala 中自定义 reduceByKey() 的并行度
val data = Seq(("a", 3), ("b", 4), ("a", 1)) 
sc.parallelize(data).reduceByKey((x, y) => x + y) // 默认并行度
sc.parallelize(data).reduceByKey((x, y) => x + y, 10) // 自定义并行度
```

在分组或者聚合操作外，Spark提供repartition() 和coalesce( )函数也能改变RDD的分区数，repartition( )函数会把数据通过IO进行shuffle操作，并创建出新的分区集合。PS：可以使用 Java 或 Scala 中的 rdd. partitions.size() 以及 Python 中的 rdd.getNumPartitions 查看 RDD 的分区数。

### 4.3.2 数据分组

如果数据已经以预期的方式提取了Key，groupByKey() 就会使用 RDD 中的键来对数据进行 分组。对于一个由类型 K 的键和类型 V 的值组成的 RDD，所得到的结果 RDD 类型会是 [K, Iterable[V]]。

groupByKey( )可以用于非<Key,Value>形式的数据；也可以根据键相同以外的条件分组——接收一个函数，对于RDD中的每个元素使用该元素，将其返回结果作为键再进行分组。

**注意：**先使用groupByKey( )然后再使用reduce( )或者fold( )的代码没有直接使用根据Key聚合的函数更高效的实现，对于Key归约数据，返回对应每个Key归约值的RDD，而不是把RDD归约为内存中的一个值。例如，rdd.reduceByKey(func) 与 rdd.groupByKey().mapValues(value => value.reduce(func)) 等价，但是前者更为高效，因为它避免了为每个键创建存放值的列表的步骤。

除了对单个RDD进行分组，cogroup() 的函数对多个共享同 一个键的 RDD 进行分组。对两个键的类型均为 K 而值的类型分别为 V 和 W 的 RDD 进行 cogroup() 时，得到的结果 RDD 类型为 [(K, (Iterable[V], Iterable[W]))]。如果其中的 一个 RDD 对于另一个 RDD 中存在的某个键没有对应的记录，那么对应的迭代器则为空。 cogroup() 提供了为多个 RDD 进行数据分组的方法。

### 4.3.3 连接

将有键的数据与另一组有键的数据一起使用是对键值对数据执行的最有用的操作之一。连 接数据可能是 pair RDD 最常用的操作之一。连接方式多种多样:**右外连接、左外连接、交叉连接以及内连接**。

**内连接**

普通的 join 操作符表示内连接 。只有在两个pairRDD中都存在的键才输出。生成的PairRDD会包含来自两个输入RDD的每一组相对应记录。

```scala
val storeAddress = sc.parallelize(Seq(("Ritual", "1026 Valencia St"), ("Philz", "748 Van Ness Ave"),("Philz", "3101 24th St"), ("Starbucks", "Seattle")))

val storeRating = sc.parallelize(Seq(("Ritual", 4.9), ("Philz", 4.8), ("a", 4.9)))

// result : [(Philz,(748 Van Ness Ave,4.8)), (Philz,(3101 24th St,4.8)), (Ritual,(1026 Valencia St,4.9))]
```

**外连接**

有时，我们不希望结果中的键必须在两个RDD中都存在。例如，在连接客户信息与推荐时，如果一些客户还没有收到推荐，我们仍然不希望丢掉这些顾客。**leftOuterJoin(otherRDD)** 和 **rightOuterJoin(otherRDD)** 都会根据键连接两个RDD，但是允许结果中存在其中的一个PairRDD所缺失的键。

- leftOuterJoin：结果RDD中，源RDD的每一个键都有对应的记录，每一个键的值是由一个源RDD中的值与另一个RDD中包含该Key的值组成的Option（在 Java 中为 Optional）对象组成的二元组。在 Python 中，如果一个值不存在，则使用 None 来表示; 而数据存在时就用常规的值来表示，不使用任何封装。
- rightOuterJoin() 几乎与 leftOuterJoin() 完全一样，只不过预期结果中的键必须出现在 第二个 RDD 中，而二元组中的可缺失的部分则来自于源 RDD 而非第二个 RDD。

参考SQL语句的Join使用方式。

### 4.3.4 数据排序

在Spark中，我们经常使用**sortByKey**([*ascending*], [*numTasks*])函数，对数据排序， ascending 参数，表 示我们是否想要让结果按升序排序(默认值为 true)。有时我们也可能想按完全不同的排 序依据进行排序。要支持这种情况，我们可以提供自定义的比较函数。

```python
# 在 Python 中以字符串顺序对整数进行自定义排序
rdd.sortByKey(ascending=True, numPartitions=None, keyfunc = lambda x: str(x))
```

```scala
// 在 Scala 中以字符串顺序对整数进行自定义排序
val input: RDD[(Int, Venue)] = ...
implicit val sortIntegersByString = new Ordering[Int] {
	override def compare(a: Int, b: Int) = a.toString.compare(b.toString)
}
rdd.sortByKey()
```

```java
// 在 Java 中以字符串顺序对整数进行自定义排序
class IntegerComparator implements Comparator<Integer> {
        public int compare(Integer a, Integer b) {
          return String.valueOf(a).compareTo(String.valueOf(b))
   }
}
rdd.sortByKey(comp)
```

## 4.5 数据分区（进阶）

在分布式程序中， 通信的代价是很大的，因此控制数据分布以获得最少的网络传输可以极大地提升整体性 能。与单台应用需要选择适当的数据结构存储数据一样，Spark 程序可以通过控制 RDD 分区方式来减少通信开销，达到优化并行程序的目的。分区并不是对所有应用都有好处的——比如，如果给定 RDD 只需要被扫描一次，我们完全没有必要对其预先进行分区处理。只有当数据集多次在 诸如连接这种基于键的操作中使用时，分区才会有帮助。

Spark 中所有的键值对 RDD 都可以进行分区，系统会根据一个针对键的函数对元素进行分组。比如，Hash分区函数与范围分区函数，

- 哈希分区将一个 RDD 分成了n个分区，此时键的哈希值对 n取模的结果相同的记录会被放在一个节点上。
- 范围分区法，将键在同一个范围区间内的记录都放在同一个节点上。

举例说明，Spark分区的原理与优化：内存中保存着一张很大的用户信息表—— 也就是一个由 (UserID, UserInfo) 对组成的RDD，其中UserInfo包含一个该用户所订阅 的主题的列表。该应用会周期性地将这张表与一个小文件进行组合，这个小文件中存着过 去五分钟内发生的事件——其实就是一个由 (UserID, LinkInfo) 对组成的表，存放着过去 五分钟内某网站各用户的访问情况。

```java
// 代码示例：用户访问其未订阅主题的页面的情况进行统计
// 初始化代码;从HDFS商的一个Hadoop SequenceFile中读取用户信息
// userData中的元素会根据它们被读取时的来源，即HDFS块所在的节点来分布
// Spark此时无法获知某个特定的UserID对应的记录位于哪个节点上
val sc = new SparkContext(...)
val userData = sc.sequenceFile[UserID, UserInfo]("hdfs://...").persist()
// 周期性调用函数来处理过去五分钟产生的事件日志
// 假设这是一个包含(UserID, LinkInfo)对的SequenceFile def processNewLogs(logFileName: String) {
val events = sc.sequenceFile[UserID, LinkInfo](logFileName)
val joined = userData.join(events)// RDD of (UserID, (UserInfo, LinkInfo)) pairs
val offTopicVisits = joined.filter {
    case (userId, (userInfo, linkInfo)) => // Expand the tuple into its components
      !userInfo.topics.contains(linkInfo.topic)
}.count()
       println("Number of visits to non-subscribed topics: " + offTopicVisits)
}
```

这段代码可以正确运行，但是不够高效。这是因为在每次调用 processNewLogs() 时都会用到join() 操作，默认情况下：连接操作会将两个数据集中的所有Key的Hash值求出来，将Hash值相同的记录通过网络传输到同一台机器上，并对键相同的记录进行连接操作。因为 userData 表比 每五分钟出现的访问日志表 events 要大得多，所以要浪费时间做很多额外工作:在每次调 用时都对 userData 表进行哈希值计算和跨节点数据混洗，虽然这些数据从来都不会变化。

<img src="./img/4-4.jpg" style="zoom: 25%;" />

​						图 4-4:未使用 partitionBy() 时对 userData 和 events 进行连接操作

要解决这一问题也很简单:在程序开始时，对 userData 表使用 partitionBy() 转化操作， 将这张表转为哈希分区。可以通过向 partitionBy 传递一个 spark.HashPartitioner 对象来 实现该操作，

```scala
// 例 4-23:Scala 自定义分区方式
val sc = new SparkContext(...)
val userData = sc.sequenceFile[UserID, UserInfo]("hdfs://...")
							.partitionBy(new HashPartitioner(100)) // 构造100个分区 
							.persist() // 持久化保存
```

processNewLogs() 方 法 可 以 保 持 不 变: 在 processNewLogs() 中，eventsRDD 是 本 地 变 量，只在该方法中使用了一次，所以为 events 指定分区方式没有什么用处。由于在构 建 userData 时调用了 partitionBy()，Spark 就知道了该 RDD 是根据键的哈希值来分 区的，这样在调用 join() 时，Spark 就会利用到这一点。具体来说，当调用 userData. join(events) 时，Spark 只会对 events 进行数据混洗操作，将 events 中特定 UserID 的记 录发送到 userData 的对应分区所在的那台机器上(见图 4-5)。这样，需要通过网络传输的 数据就大大减少了，程序运行速度也可以显著提升了。

注意，partitionBy() 是一个转化操作，因此它的返回值是新RDD，但它不会改变原来的 RDD（RDD 一旦创建就无法修改）。因此应该对 partitionBy() 的结果进行持久化， 并保存为 userData，而不是原来的 sequenceFile() 的输出。此外，传给 partitionBy() 的 100 表示分区数目，它会控制之后对这个 RDD 进行进一步操作(比如连接操作)时有多少 任务会并行执行。总的来说，这个值至少应该和集群中的总核心数一样。

![](./img/4-5.jpg)

​									图 4-5:使用 partitionBy() 时对 userData 和 events 进行连接操作

**注意：**如果没有将 partitionBy() 转化操作的结果持久化，那么后面每次用到这个 RDD 时都会重复地对数据进行分区操作。不进行持久化会导致整个 RDD 谱 系图重新求值。那样的话，partitionBy() 带来的好处就会被抵消，导致重 复对数据进行分区以及跨节点的混洗，和没有指定分区方式时发生的情况十 分相似。

其他Spark操作也会自动为结果RDD设定已知的分区方式信息，例如：sortByKey( )和groupByKey( )会分别生成范围分区的RDD和Hash分区的RDD。

**Tips**

Spark 的 Java 和 Python 的 API 都和 Scala 的一样，可以从数据分区中获益。 不过，在 Python 中，你不能将 HashPartitioner 对象传给 partitionBy，而 只需要把需要的分区数传递过去(例如 rdd.partitionBy(100))。

### 4.5.1 获取RDD的分区方式

在Java和Scala中，可以使用RDD的partitioner属性(Java使用partition er( )方法)来获取RDD的分区方式。它会返回一个scala.Option对象，这是 Scala 中用来存放 可能存在的对象的容器类。你可以对这个Option对象调用 isDefined() 来检查其中是否有值，调用 get() 来获取其中的值。

```powershell
scala> val pairs = sc.parallelize(List((1, 1), (2, 2), (3, 3)))
     pairs: spark.RDD[(Int, Int)] = ParallelCollectionRDD[0] at parallelize at
     <console>:12
// Scala获取RDD分区，初始时没有分区方式信息(返回值为 None 的 Option 对象)
scala> pairs.partitioner
     res0: Option[spark.Partitioner] = None
// Scala设置分区     
scala> val partitioned = pairs.partitionBy(new spark.HashPartitioner(2))
     partitioned: spark.RDD[(Int, Int)] = ShuffledRDD[1] at partitionBy at <console>:14
scala> partitioned.partitioner
     res1: Option[spark.Partitioner] = Some(spark.HashPartitioner@5147788d)

```

### 4.5.2 从分区中获益操作

Spark 的许多操作都引入了将数据根据键跨节点进行混洗的过程，所有这些操作都会 从数据分区中获益(可以通过指定分区的方式尽量减少Shuffle)。能够从数据分区中获益的操作有 cogroup()、 groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、 combineByKey() 以及 lookup()。

### 4.5.3 影响分区方式的操作

Spark内部知道各操作会如何影响分区，并将会对数据进行分区操作的结果RDD自动设置为对应的分区器。例如，如果你调用 join() 来连接两个 RDD;由于键相同的元素 会被哈希到同一台机器上，Spark 知道输出结果也是哈希分区的，这样对连接的结果进行 诸如 reduceByKey() 这样的操作时就会明显变快。

这里列出了所有会为生成的结果 RDD 设好分区方式的操作:cogroup()、groupWith()、 join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、 combineByKey()、partitionBy()、sort()、mapValues()(如果父 RDD 有分区方式的话)、 flatMapValues()(如果父 RDD 有分区方式的话)，以及 filter()(如果父 RDD 有分区方 式的话)。其他所有的操作生成的结果都不会存在特定的分区方式。

对于Key，Value的二元组操作，结果RDD的分区方式取决于父RDD的分区方式，默认Hash分区，分区数量与操作并行度一致。如果两个父 RDD 都设置过分区方 式，结果 RDD 会采用第一个父 RDD 的分区方式。

### 4.5.4 PageRank算法

PageRank是执行多次连接的一个**迭代算法**，算法会维护两个数据集：

- (page, linkList)，包含每个页面和相邻页面列表
- (page, rank)，包含每个页面的当前排序值

算法具体执行步骤如下：

  1. 将每个页面的排序值初始化为1.0

  2. 在每次迭代中，对页面 p，向其每个相邻页面(有直接链接的页面)发送一个值为 rank(p)/numNeighbors(p) 的贡献值。

  3. 将每个页面的排序值设为 0.15 + 0.85 * contributionsReceived。

  4. 迭代循环直至收敛

	```scala
	// 假设相邻页面列表以Spark objectFile的形式存储
	val links = sc.objectFile[(String, Seq[String])]("links")
	                   .partitionBy(new HashPartitioner(100))
	                   .persist()
	// 将每个页面的排序值初始化为1.0;由于使用mapValues，生成的RDD // 的分区方式会和"links"的一样
	var ranks = links.mapValues(v => 1.0)
	// 运行10轮PageRank迭代 
	for(i <- 0 until 10) {
	  // links与rank join，获取每个页面对应的相邻页面列表和排序值
	    val contributions = links.join(ranks).flatMap {
	      //flatMap 创建出“contributions”来记录每个页面对各相邻页面的贡献。
	      case (pageId, (links, rank)) =>
	      links.map(dest => (dest, rank / links.size))
	}
	ranks = contributions.reduceByKey((x, y) => x + y).mapValues(v => 0.15 + 0.85*v)
	}
	// 写出最终排名 
	ranks.saveAsTextFile("ranks")
	```

	​		示例程序还是做了不少事情来确保 RDD 以比较高效的方式进行分区，以最小化通信开销:

	1. linksRDD 在每次迭代中都会和 ranks 发生连接操作。由于 links 是一个静态 数据集，所以我们在程序一开始的时候就对它进行了分区操作，这样就不需要把它通过 网络进行数据混洗了。这一优化相比 PageRank 的原始实现(例如普通的 MapReduce)节约了相当可观的网络通信开销。MapReduce计算框架将中间计算结果保存在临时磁盘中，迭代计算会产生大量的网络IO，磁盘IO资源浪费
	2. 出于同样的原因，我们调用 links 的 persist() 方法，将它保留在内存中以供每次迭代 使用。
	3. 当我们第一次创建 ranks 时，我们使用 mapValues() 而不是 map() 来保留父 RDD(links) 的分区方式，这样对它进行的第一次连接操作就会开销很小。
	4. 在循环体中，我们在 reduceByKey() 后使用 mapValues();因为 reduceByKey() 的结果已 经是哈希分区的了，这样一来，下一次循环中将映射操作的结果再次与 links 进行连接 操作时就会更加高效。

	**tips**

	​		为了最大化分区相关优化的潜在作用，无需改变元素的Key时，尽量使用mapValues() 或 flatMapValues()。

### 4.5.5 自定义分区方式

除了使用Spark提供的HashPartitioner( )和RangePartitioner( )，还可以自定义分区Partitioner对象。继承org.apache.spark.Partitioner类并实现以下三个方法

- numPatitions：Int，返回创建的分区数
- getPartition(key: Any)：Int:返回给定键的分区编号(0 到 numPartitions-1)。
- equals()：**非常重要**，Spark 需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样 Spark 才可以判断两个 RDD 的分区方式是否相同。

**注意**：在Java中，当自定义分区器依赖于对象的hashCode方法，必须重新实现hashCode( )方法。

使用自定义的 Partitioner 是很容易的:只要把它传给 partitionBy() 方法即可。Spark 中 有许多依赖于数据混洗的方法，比如 join() 和 groupByKey()，它们也可以接收一个可选的 Partitioner 对象来控制输出数据的分区方式。

在 Python 中，不需要扩展 Partitioner 类，而是把一个特定的哈希函数作为一个额外的参数传给RDD.partitionBy() 函数。

```python
# Python 自定义分区方式
import urlparse
def hash_domain(url):
		return hash(urlparse.urlparse(url).netloc)
rdd.partitionBy(20, hash_domain) # 创建20个分区
```

**注意**：python中传过去的哈希函数会被与其他 RDD 的分区函数区分开来。如果你想要对 多个 RDD 使用相同的分区方式，就应该使用同一个函数对象，比如一个全局函数，而不 是为每个 RDD 创建一个新的函数对象。