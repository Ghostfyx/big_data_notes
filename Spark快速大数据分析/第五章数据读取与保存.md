# 第五章 数据读取与保存

本章对于工程师和数据科学家都较为实用。工程师会了解到更多的输出格式，有利于找到非常适合用于下游处理程序的格式。数据科学家则可能更关心数据的现有的组织形式。

##5.1 动机

Spark支持很多输入输出源。一部分原始时Spark本身是基于Hadoop生态圈而构建，特别是Spark可以通过Hadoop的InputFormat和OutputFormat接口访问数据，大部分文件格式与存储系统都支持这样的接口，例如：S3，HDFS，Cassandra，HBase等。

本章会介绍三类常见的数据源：

- 文件格式与文件系统

	对于存储在本地文件系统或分布式文件系统(NFS，HDFS，Amazon S3等)中的数据，Spark可以访问多种文件格式的数据：文本文件，Json，SequenceFile，以及protocol buffer。

- Spark SQL中的结构化数据

	针对Json和Hive在内的结构化数据。

- 数据库与键值存储

	Spark自带库和一些第三方库，可以用来连接Cassandra、HBase、Elasticsearch以及JDBC源。

## 5.2 文件格式

Spark支持从诸如文本文件的非结构化的文件， 到诸如 JSON 格式的半结构化的文件，再到诸如 SequenceFile 这样的结构化的文件。

​															**表5-1:Spark支持的一些常见格式**

| 格式名称         | 结构化   | 备注                                                         |
| ---------------- | -------- | ------------------------------------------------------------ |
| 文本文件         | 否       | 普通文本文件，每一行一条记录                                 |
| JSON             | 半结构化 | 大多数库都要求每行一条记录                                   |
| CSV              | 是       | 通常用于电子表格                                             |
| SequenceFiles    | 是       | 用于**键值对**数据的Hadoop文件格式                           |
| Protocol buffers | 是       | 一种快速、节约空间的跨语言格式                               |
| 对象文件         | 是       | 用于将Spark作业中的数据存储下来以让共享的读取，改变类时会失效，因为它依赖于 Java 序列化 |

### 5.2.1 文本文件

Spark将文本文件的每一行读取为RDD的一个元素，也可以将多个文本文件读取为PairRDD，文件名为Key，文件内容为Value。

#### 1. 读取文本文件

使用SparkContext的textFile(filePath, minPartitions)读取文本文件。

```python
# 例 5-1:在 Python 中读取一个文本文件
input = sc.textFile("file:///home/holden/repos/spark/README.md")
```

```scala
// 例 5-2:在 Scala 中读取一个文本文件
val input = sc.textFile("file:///home/holden/repos/spark/README.md")
```

```java
// 例 5-3:在 Java 中读取一个文本文件
JavaRDD<String> input = sc.textFile("file:///home/holden/repos/spark/README.md")
```

多个输入文件以目录的形式出现，可以用两种方式来读取：

- textFile函数，文件路径入参为目录，会讲所有文件中的数据读取到RDD
- wholeTextFiles函数，返回PairRDD，Key时文件名，Value是文件内容，wholeTextFiles适合处理时间序列的数据（将数据分时间段存储在不同文件中）

#### 2. 文件路径

spark支持：

1. 指定文件所在路径+文件名读取单个文件；
2. 指定文件目录路径，读取多个文件；
3. 文件路径使用通配符(如 part-*.txt)。大规模数据集通常存放在多个文件中，尤其是在同一目录中存在一些别的文件(比如成功标记文件)的时候。

#### 3. 保存文本文件

使用saveTextFile( )方法，文件目录作为入参，将RDD中内容作为存储为多个文件在路径下。在这个方法中，我 们不能控制数据的哪一部分输出到哪个文件中，不过有些输出格式支持控制。

### 5.2.2  JSON

JSON是一种广泛使用的半结构化数据格式。读取 JSON 数据的最简单的方式是将数据作为文本文件读取，然后使用 JSON 解析器来对 RDD 中的值进行映射操作。在 Java 和 Scala 中也可以使用一个自定义 Hadoop 格式来操作 JSON 数据，使用的语言中 构建一个 JSON 解析器的开销较大，你可以使用 mapPartitions() 来重用解析器。另外Spark SQL也可以读取JSON数据。

#### 1. 读取JSON

将数据作为文本文件读取，然后对JSON数据进行解析，这种方法假设文件中的每一行都是一个JSON串，如果你有跨行的 JSON 数据，你就只能读入整个文件，然后对每个文件进行解析。

python，Java和Scala有大量可用的第三方JSON解析库可以用于JSON解析。例如：python自带JSON库，Java中的FastJson，JackSon等。

```python
# 例 5-6:在 Python 中读取非结构化的 JSON 
import json
data = input.map(lambda x: json.loads(x))
```

在Scala和Java中，通常将记录读入到一个代表结构信息的类中。

```scala
// 例 5-7:在 Scala 中读取 JSON
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.databind.DeserializationFeature
...
case class Person(name: String, lovesPandas: Boolean) // 必须是顶级类
...
// 将其解析为特定的case class。使用flatMap，通过在遇到问题时返回空列表(None) // 来处理错误，而在没有问题时返回包含一个元素的列表(Some(_))
val result = input.flatMap(record => {
       try {
         Some(mapper.readValue(record, classOf[Person]))
       } catch {
         case e: Exception => None
}})
```

```java
//  例 5-8:在 Java 中读取 JSON
class ParseJson implements FlatMapFunction<Iterator<String>, Person> {
       public Iterable<Person> call(Iterator<String> lines) throws Exception {
         ArrayList<Person> people = new ArrayList<Person>();
         ObjectMapper mapper = new ObjectMapper();
         while (lines.hasNext()) {
           String line = lines.next();
           try {
             people.add(mapper.readValue(line, Person.class));
           } catch (Exception e) {
								// 跳过失败的数据 
           }
         }
         return people;
       }
}
JavaRDD<String> input = sc.textFile("file.json");
JavaRDD<Person> result = input.mapPartitions(new ParseJson());
```

**注意：**上面代码中，有数据转换异常捕获非常重要，对于读取JSON这样半结构化的数据。小数据集可以接受在遇到错误的输入时停止程序(程序失败)，但是对于大规模数据集来说，格式错误是家常便饭。捕获异常时不中断程序，使用累加器来跟踪错误的个数。

#### 2. 保存JSON

使用第三方JSON库或自带JSON库，将结构化数据组成的RDD转换为字符串RDD，最后使用Spark的文本文件API写入。

```python
# 例 5-9:在 Python 保存为 JSON
data.filter(lambda x: x["lovesPandas"]).map(lambda x: json.dumps(x))
       .saveAsTextFile(outputFile)
```

```scala
// 例 5-10:在 Scala 中保存为 JSON
result.filter(p => P.lovesPandas).map(mapper.writeValueAsString(_))
       .saveAsTextFile(outputFile)
```

```java
//
class WriteJson implements FlatMapFunction<Iterator<Person>, String> {
       public Iterable<String> call(Iterator<Person> people) throws Exception {
         ArrayList<String> text = new ArrayList<String>();
         ObjectMapper mapper = new ObjectMapper();
         while (people.hasNext()) {
           Person person = people.next();
           text.add(mapper.writeValueAsString(person));
           }
         return text;
       }
}
JavaRDD<Person> result = input.mapPartitions(new ParseJson()).filter(
       new LikesPandas());
JavaRDD<String> formatted = result.mapPartitions(new WriteJson());
     formatted.saveAsTextFile(outfile);
```

### 5.2.3 逗号分隔值与制表符分隔值

逗号分隔值文件（CSV）每一行有固定数目的字段，字段间用逗号隔开；制表符分隔值（TSV文件）字段间使用制表符分隔，通常是每行数据对应一条记录。CSV 文件和 TSV 文件有时支持的标准并不一致，主要是在处理换行符、转义字符、非 ASCII字符、非整数值等方面。CSV 原生并不支持嵌套字段，所以需要手动组合 和分解特定的字段。

与JSON中的字段不一样的是，每行记录没有关联的字段名，通常做法是使用第一行的值作为字段名。

#### 1. 读取CSV文件

读取 CSV/TSV 数据和读取 JSON 数据相似，都需要先把文件当作普通文本文件来读取数 据，再对数据进行处理。与 JSON 一样，CSV 也有很多不同的库，例如：python自带的CSV库，Java和Scala第三方opencsv库。

**Tips**

Hadoop InputFormat 中的 CSVInputFormat也可以用于在 Scala 和 Java 中读取 CSV 数据。不过它不支持包含换行符的记录。

下面示例(5-12至5-14)是CSV文件中所有字段均不包括换行符。

```python
# 例 5-12:在 Python 中使用 textFile() 读取 CSV 
import csv
import StringIO
def loadRecord(line):
		"""解析一行CSV记录"""
		input = StringIO.StringIO(line)
		reader = csv.DictReader(input, fieldnames=["name", "favouriteAnimal"]) 
		return reader.next()
input = sc.textFile(inputFile).map(loadRecord)
```

```scala
//例 5-13:在 Scala 中使用 textFile() 读取 CSV
import Java.io.StringReader
import au.com.bytecode.opencsv.CSVReader
...
val input = sc.textFile(inputFile)
val result = input.map{ line =>
     val reader = new CSVReader(new StringReader(line));
     reader.readNext();
}
```

```java
// 例 5-14:在 Java 中使用 textFile() 读取 CSV
import au.com.bytecode.opencsv.CSVReader;
import Java.io.StringReader;
...
public static class ParseLine implements Function<String, String[]> {
       public String[] call(String line) throws Exception {
         CSVReader reader = new CSVReader(new StringReader(line));
         return reader.readNext();
       } 
}
JavaRDD<String> csvFile1 = sc.textFile(inputFile);
JavaPairRDD<String[]> csvData = csvFile1.map(new ParseLine());
```

如果字段中包含有换行符，需要完整读入文件，然后解析各个字段，如果每个文件很大，读取和解析过程会成为性能瓶颈。如例 5-15 至例 5-17 所示。

**Tips 换行符**

在windows下：**\r\n**代表换行，拆分两个代码是：回到行首+换到下一行。在linux下：**\n**代表换行。

```python
# 例 5-15:在 Python 中完整读取 CSV
import csv
def loadRecords(fileNameContents):
	"""读取给定文件中的所有记录"""
	input = StringIO.StringIO(fileNameContents[1])
	reader = csv.DictReader(input, fieldnames=["name", "favoriteAnimal"]) 
  return reader
fullFileData = sc.wholeTextFiles(inputFile).flatMap(loadRecords)
```

```scala
// 例 5-16:在 Scala 中完整读取 CSV
case class Person(name: String, favoriteAnimal: String)
val input = sc.wholeTextFiles(inputFile)
val result = input.flatMap{ case (_, txt) =>
val reader = new CSVReader(new StringReader(txt));
    reader.readAll().map(x => Person(x(0), x(1)))
}
```

```java
 // 例 5-17:在 Java 中完整读取 CSV
public static class ParseLine implements FlatMapFunction<Tuple2<String, String>, String[]> {
       public Iterable<String[]> call(Tuple2<String, String> file) throws Exception {
         CSVReader reader = new CSVReader(new StringReader(file._2()));
         return reader.readAll();
       }
}
JavaPairRDD<String, String> csvData = sc.wholeTextFiles(inputFile);
JavaRDD<String[]> keyedRDD = csvData.flatMap(new ParseLine());
```

#### 2. 保存CSV

