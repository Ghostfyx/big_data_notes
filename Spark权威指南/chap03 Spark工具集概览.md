# ç¬¬ä¸‰ç«  Sparkå·¥å…·é›†æ¦‚è§ˆ

åœ¨ç¬¬äºŒç« ä¸­ï¼Œåœ¨Sparkç»“æ„åŒ–APIä¸­å¼•å…¥Sparkçš„æ ¸å¿ƒæ¦‚å¿µï¼Œæ¯”å¦‚transformationå’Œactionæ“ä½œã€‚è¿™äº›ç®€å•çš„æ¦‚å¿µæ¨¡å—æ˜¯Apache Sparkåºå¤§çš„å·¥å…·å’Œåº“ç”Ÿæ€ç³»ç»Ÿçš„åŸºç¡€ï¼Œå¦‚å›¾3-1æ‰€ç¤ºï¼ŒSpark æ˜¯ç”±è¿™äº›åŸå§‹çš„(åº•å±‚apiå’Œç»“æ„åŒ–çš„api)ç»„æˆçš„ï¼Œç„¶åæ˜¯ä¸€ç³»åˆ—ç”¨äºé™„åŠ åŠŸèƒ½çš„æ ‡å‡†åº“ã€‚ 

![](./img/3-1.jpg)

â€‹															**å›¾3-1 Spark APiä¸æ ‡å‡†ç±»åº“**

Sparkçš„åº“æ”¯æŒå„ç§ä¸åŒçš„è®¡ç®—ä»»åŠ¡ï¼Œä»å›¾è®¡ç®—å’Œæœºå™¨å­¦ä¹ åˆ°æµå¤„ç†ï¼Œåˆ°ä¸å…¶ä»–å¤§é‡çš„è®¡ç®—æœºé›†ç¾¤å’Œå­˜å‚¨ç³»ç»Ÿé›†æˆã€‚æœ¬ç« åŒ…æ‹¬ä»¥ä¸‹å†…å®¹: 

- ä½¿ç”¨Spark-submitè¿è¡Œç”Ÿäº§åº”ç”¨ç¨‹åº
- Datasetï¼šç”¨äºç»“æ„åŒ–æ•°æ®çš„ç±»å‹å®‰å…¨API
- ç»“æ„åŒ–æµå¤„ç†
- æœºå™¨å­¦ä¹ ä¸é«˜çº§åˆ†æ
- å¼¹æ€§æ•°æ®é›†(RDD)ï¼šSparkçš„ä½çº§API
- SparkR
- ç¬¬ä¸‰æ–¹ç”Ÿæ€ç³»ç»Ÿ

## 3.1 ç”Ÿäº§ç¨‹åºè¿è¡Œ

ä½¿ç”¨Spark-submitå°†ç¨‹åºä»£ç å‘é€åˆ°é›†ç¾¤å¹¶å¯åŠ¨æ‰§è¡Œï¼Œæäº¤åï¼Œåº”ç”¨ç¨‹åºå°†è¿è¡Œï¼Œç›´åˆ°å®ƒé€€å‡º(å®Œæˆä»»åŠ¡)æˆ–é‡åˆ°é”™è¯¯ã€‚å¯ä»¥ä½¿ç”¨Spark çš„æ‰€æœ‰æ”¯æŒé›†ç¾¤ç®¡ç†å™¨ï¼ŒåŒ…æ‹¬ standaloneã€Mesoså’Œyarnã€‚ 

spark-submitæä¾›äº†å‡ ä¸ªæ§åˆ¶é¡¹ï¼Œå¯ä»¥åœ¨å…¶ä¸­æŒ‡å®šåº”ç”¨ç¨‹åºéœ€è¦çš„èµ„æºä»¥åŠå®ƒåº”è¯¥å¦‚ä½•è¿è¡Œä»¥åŠå®ƒçš„
å‘½ä»¤è¡Œå‚æ•°ã€‚ å¯ä»¥åœ¨Spark çš„ä»»ä½•æ”¯æŒçš„è¯­è¨€ä¸­ç¼–å†™åº”ç”¨ç¨‹åºï¼Œç„¶åæäº¤å®ƒä»¬æ‰§è¡Œã€‚æœ€ç®€å•çš„ç¤ºä¾‹æ˜¯åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œåº”ç”¨ç¨‹åºã€‚

```sh
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local \
  ./examples/jars/spark-examples_2.11-2.2.0.jar 10
```

è¿è¡ŒPythonï¼š

```sh
./bin/spark-submit \
  --master local \
  ./examples/src/main/python/pi.py 10
```

é€šè¿‡æ›´æ”¹Spark-submitçš„masterå‚æ•°ï¼Œå¯ä»¥å°†åº”ç”¨ç¨‹åºæäº¤åˆ°Sparkçš„standaloneé›†ç¾¤èµ„æºç®¡ç†å™¨ã€Mesosæˆ–Yarnçš„é›†ç¾¤ç®¡ç†å™¨ã€‚

## 3.2 Datasets:ç”¨äºç»“æ„åŒ–æ•°æ®çš„ç±»å®‰å…¨ç±»å‹API

Datesetsæ˜¯Sparkçš„ç»“æ„åŒ–æ•°æ®çš„ç±»å®‰å…¨ç±»å‹APIã€‚Sparkå½“å‰ä»…æ”¯æŒJavaå’ŒScalaè¯­è¨€ï¼Œä¸æ”¯æŒPythonä¸Rï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŠ¨æ€ç±»å‹ã€‚

ä¸Šä¸€ç« ä»‹ç»çš„DataFramesï¼Œæ˜¯ç±»å‹è¡Œçš„å¯¹è±¡ï¼Œå¯ä»¥å®¹çº³ä¸åŒç±»å‹çš„è¡¨æ ¼æ•°æ®çš„åˆ†å¸ƒå¼é›†åˆã€‚Dataset APIèƒ½å¤Ÿå°†Java/Scalaç±»åˆ†é…ç»™DataFrameä¸­çš„è®°å½•ï¼Œå¹¶å°†å…¶ä½œä¸ºç±»å‹åŒ–å¯¹è±¡çš„é›†åˆè¿›è¡Œæ“ä½œï¼Œç±»ä¼¼äºjavaçš„arrayListæˆ–Scala Seqã€‚Datasets apiæ˜¯ç±»å‹å®‰å…¨çš„ï¼Œä¸èƒ½å°†ä¸Datasetsæœ€åˆæ•°æ®ç±»å‹ä¸åŒçš„ç±»æ”¾å…¥ã€‚è¿™ä½¿å¾—DataSetsé€‚ç”¨äºç¼–å†™å¤§å‹åº”ç”¨ï¼Œè´Ÿè´£ä¸åŒæ¨¡å—çš„å¼€å‘äººå‘˜å¿…é¡»å®šä¹‰å¥½æ¥å£ä¹‹é—´çš„äº¤äº’ã€‚

DataSetç±»çš„å‚æ•°ï¼š`Dataset[T]` in Java and `Dataset[T]` in Scalaã€‚ä¾‹å¦‚ï¼š`DataSet[Person]`å°†ä¿è¯åŒ…å«Personç±»çš„å¯¹è±¡ã€‚ä»Spark 2.0å¼€å§‹ï¼Œæ”¯æŒçš„ç±»å‹æ˜¯Javaä¸­éµå¾ªJavaBeanæ¨¡å¼çš„ç±»å’ŒScalaä¸­çš„caseç±»ã€‚è¿™äº›ç±»å‹å—åˆ°é™åˆ¶ï¼Œå› ä¸ºSparkéœ€è¦èƒ½å¤Ÿè‡ªåŠ¨åˆ†æç±»å‹Tå¹¶ä¸ºæ•°æ®é›†ä¸­çš„è¡¨æ ¼æ•°æ®åˆ›å»ºé€‚å½“çš„æ¨¡å¼ã€‚

Datasetå¦ä¸€ä¼˜ç‚¹æ˜¯ï¼šåªæœ‰åœ¨éœ€è¦æˆ–æƒ³è¦æ—¶æ‰èƒ½ä½¿ç”¨å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œå°†å®šä¹‰è‡ªå·±çš„æ•°æ®ç±»å‹ï¼Œå¹¶é€šè¿‡ä»»æ„æ˜ å°„å’Œç­›é€‰å‡½æ•°å¯¹å…¶è¿›è¡Œæ“ä½œã€‚åœ¨å®Œæˆæ“ä½œä¹‹åï¼ŒSparkå¯ä»¥è‡ªåŠ¨åœ°å°†å®ƒè½¬æ¢å›æ•°æ®å¸§ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ä½¿ç”¨SparkåŒ…å«çš„æ•°ç™¾ä¸ªå‡½æ•°æ¥è¿›ä¸€æ­¥æ“ä½œå®ƒã€‚è¿™ä½¿å¾—å®ƒå¾ˆå®¹æ˜“ä¸‹é™åˆ°è¾ƒä½çš„çº§åˆ«ï¼Œåœ¨å¿…è¦æ—¶æ‰§è¡Œç±»å‹å®‰å…¨ï¼›å¹¶å‘ä¸ŠæŠ½è±¡åˆ°æ›´é«˜çš„SQLä»¥è¿›è¡Œæ›´å¿«é€Ÿçš„åˆ†æã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå°ç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ç±»å‹å®‰å…¨å‡½æ•°å’Œç±»ä¼¼æ•°æ®å¸§çš„SQLè¡¨è¾¾å¼å¿«é€Ÿç¼–å†™ä¸šåŠ¡é€»è¾‘ï¼š

```scala
case class Flight(DEST_COUNTRY_NAME: String,
                  ORIGIN_COUNTRY_NAME: String,
                  count: BigInt)
val flightsDF = spark.read
  .parquet("/data/flight-data/parquet/2010-summary.parquet/")
val flights = flightsDF.as[Flight]
```

æœ€åä¸€ä¸ªä¼˜ç‚¹æ˜¯ï¼Œå½“è°ƒç”¨collectæˆ–æ¥æ”¶Datasetæ—¶ï¼Œå®ƒå°†åœ¨Datasetä¸­è½¬æ¢é€‚å½“ç±»å‹çš„å¯¹è±¡ï¼Œè€Œä¸æ˜¯DataFrameè¡Œã€‚è¿™ä½¿å¾—å¾ˆå®¹æ˜“è·å¾—ç±»å‹å®‰å…¨æ€§ï¼Œå¹¶ä»¥åˆ†å¸ƒå¼å’Œæœ¬åœ°æ–¹å¼å®‰å…¨åœ°æ‰§è¡Œæ“ä½œï¼Œè€Œæ— éœ€æ›´æ”¹ä»£ç ï¼š

```scala
flights
  .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(flight_row => flight_row)
  .take(5)

flights
  .take(5)
  .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(fr => Flight(fr.DEST_COUNTRY_NAME, fr.ORIGIN_COUNTRY_NAME, fr.count + 5))
```

## 3.3 Structured Streaming

Structured Streamingæ˜¯ä¸€ç§ç”¨äºæµå¤„ç†çš„é«˜çº§APIï¼Œé€‚ç”¨äºSpark 2.2ä¹‹åçš„ç‰ˆæœ¬ã€‚ç»“æ„åŒ–æµå¤„ç†å…è®¸ä½¿ç”¨Sparkçš„ç»“æ„åŒ–APIåœ¨æ‰¹å¤„ç†æ¨¡å¼ä¸‹çš„æ“ä½œï¼Œå¹¶ä»¥æµæ–¹å¼è¿è¡Œå®ƒä»¬ã€‚è¿™å¯ä»¥å‡å°‘å»¶è¿Ÿå¹¶å…è®¸å¢é‡å¤„ç†ã€‚

ç»“æ„åŒ–æµå¤„ç†æœ€å¤§çš„ä¼˜ç‚¹æ˜¯ï¼šåœ¨ä¸æ”¹å˜ä»£ç çš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿåœ°ä»æµå¼æ•°æ®ç³»ç»Ÿä¸­æå–ä»·å€¼æ•°æ®ã€‚ä»¥æ‰¹å¤„ç†ä½œä¸šä½œä¸ºåŸå‹ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºæµå¼ä½œä¸šã€‚æ‰€æœ‰è¿™äº›å·¥ä½œçš„æ–¹å¼éƒ½æ˜¯é€šè¿‡å¢é‡å¤„ç†è¿™äº›æ•°æ®ã€‚

çœ‹ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œè¯´æ˜å¼€å§‹ä½¿ç”¨ç»“æ„åŒ–æµæ˜¯å¤šä¹ˆå®¹æ˜“ã€‚å°†ä½¿ç”¨ä¸€ä¸ªé›¶å”®æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æœ‰ç‰¹å®šçš„æ—¥æœŸå’Œæ—¶é—´ä¾›æˆ‘ä»¬ä½¿ç”¨ã€‚å°†ä½¿ç”¨â€œæŒ‰å¤©â€æ–‡ä»¶é›†ï¼Œå…¶ä¸­ä¸€ä¸ªæ–‡ä»¶è¡¨ç¤ºä¸€å¤©çš„æ•°æ®ã€‚

é¦–å…ˆå°†å®ƒæ”¾åœ¨è¿™ç§æ ¼å¼ä¸­ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒè¿›ç¨‹ä»¥ä¸€è‡´å’Œè§„åˆ™çš„æ–¹å¼ç”Ÿæˆçš„æ•°æ®ï¼Œå¹¶å‘é€åˆ°ä¸€ä¸ªä½ç½®ï¼Œåœ¨é‚£é‡Œä½¿ç”¨Structured Streamingä½œä¸šå°†è¯»å–è¿™äº›æ•°æ®ã€‚

```
InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country
536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,2010-12-01 08:26:00,2.55,17...
536365,71053,WHITE METAL LANTERN,6,2010-12-01 08:26:00,3.39,17850.0,United Kin...
536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,2010-12-01 08:26:00,2.75,17850...
```

é¦–å…ˆå°†æ•°æ®ä½œä¸ºé™æ€Datasetè¿›è¡Œåˆ†æï¼Œå¹¶åˆ›å»ºä¸€ä¸ªDataFrameæ¥æ‰§è¡Œæ­¤æ“ä½œï¼š

**Scalaå®ç°**

```scala
val staticDataFrame = spark.read.format("csv")
  .option("header", "true")
  .option("inferSchema", "true")
  .load("/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
val staticSchema = staticDataFrame.schema
```

**Pythonå®ç°**

```python
staticDataFrame = spark.read.format("csv")\
  .option("header", "true")\
  .option("inferSchema", "true")\
  .load("/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
staticSchema = staticDataFrame.schema
```

ç”±äºä½¿ç”¨çš„æ˜¯æ—¶é—´åºåˆ—æ•°æ®ï¼Œå› æ­¤åº”è¯¥å¦‚ä½•åˆ†ç»„å’Œèšåˆæ•°æ®ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œå°†æŸ¥çœ‹ç»™å®šå®¢æˆ·ï¼ˆCustomerIdï¼‰è¿›è¡Œå¤§é¢è´­ä¹°çš„é”€å”®æ—¶é—´ã€‚ä¾‹å¦‚ï¼Œæ·»åŠ ä¸€ä¸ªtotal coståˆ—ï¼Œçœ‹çœ‹å®¢æˆ·åœ¨å“ªä¸€å¤©èŠ±è´¹æœ€å¤šã€‚

`window()`å‡½æ•°ç†æ—¥æœŸå’Œæ—¶é—´æˆ³çš„æœ‰ç”¨å·¥å…·ï¼ŒåŒ…å«èšåˆä¸­æ¯å¤©çš„æ‰€æœ‰æ•°æ®ã€‚æ˜¯æ•°æ®ä¸­æ—¶é—´åºåˆ—çš„ä¸€ä¸ªçª—å£ã€‚ä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š

**Scala**

```scala
import org.apache.spark.sql.functions.{window, column, desc, col}
staticDataFrame
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))
  .sum("total_cost")
  .show(5)
```

**Python**

```python
from pyspark.sql.functions import window, column, desc, col
staticDataFrame\
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")\
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
  .sum("total_cost")\
  .show(5)
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```
+----------+--------------------+------------------+
|CustomerId|              window|   sum(total_cost)|
+----------+--------------------+------------------+
|   17450.0|[2011-09-20 00:00...|          71601.44|
...
|      null|[2011-12-08 00:00...|31975.590000000007|
+----------+--------------------+------------------+
```

æµå¼å¤„ç†ä»£ç ä¸æ‰¹å¤„ç†ä»£ç å®é™…ä¸Šå‡ ä¹æ²¡æœ‰ä»€ä¹ˆå˜åŒ–ã€‚æœ€å¤§çš„å˜åŒ–æ˜¯ä½¿ç”¨readStreamè€Œä¸æ˜¯readï¼Œå¦å¤–maxFilesPerTriggeré€‰é¡¹ï¼ŒæŒ‡å®šåº”è¯¥ä¸€æ¬¡è¯»å…¥çš„æ–‡ä»¶æ•°ã€‚è¿™æ˜¯ä¸ºäº†æ›´å¥½æ¼”ç¤ºæ•°æ®æµï¼Œåœ¨ç”Ÿäº§åœºæ™¯ä¸­ï¼Œè¿™å¯èƒ½ä¼šè¢«å¿½ç•¥ã€‚

**Scala**

```scala
val streamingDataFrame = spark.readStream
    .schema(staticSchema)
    .option("maxFilesPerTrigger", 1)
    .format("csv")
    .option("header", "true")
    .load("/data/retail-data/by-day/*.csv")
```

**Python**

```python
streamingDataFrame = spark.readStream\
    .schema(staticSchema)\
    .option("maxFilesPerTrigger", 1)\
    .format("csv")\
    .option("header", "true")\
    .load("/data/retail-data/by-day/*.csv")
```

æµå¼æ•°æ®çš„æ‰¹å¤„ç†é€»è¾‘ï¼š

**scala**

```scala
val purchaseByCustomerPerHour = streamingDataFrame
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")
  .groupBy(
    $"CustomerId", window($"InvoiceDate", "1 day"))
  .sum("total_cost")
```

**Python**

```python
purchaseByCustomerPerHour = streamingDataFrame\
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")\
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
  .sum("total_cost")
```

è¿™ä»ç„¶æ˜¯ä¸€ä¸ªæƒ°æ€§æ“ä½œï¼Œå› æ­¤éœ€è¦è°ƒç”¨æµæ“ä½œæ¥å¼€å§‹æ‰§è¡Œæ­¤æ•°æ®æµã€‚æµå¼å¤„ç†ä¸æ‰¹å¤„ç†çš„actionæ“ä½œæœ‰æ‰€ä¸åŒï¼Œå› ä¸ºæµæ“ä½œå°†åœ¨æŸå¤„å¡«å……æ•°æ®ï¼Œè€Œä¸æ˜¯ä»…ä»…è°ƒç”¨countä¹‹ç±»çš„æ“ä½œï¼ˆè¿™åœ¨æµä¸Šæ— è®ºå¦‚ä½•éƒ½æ²¡æœ‰ä»»ä½•æ„ä¹‰ï¼‰ã€‚æµå¼å¤„ç†çš„è¡ŒåŠ¨æ“ä½œå°†è½¬æ¢æ“ä½œç»“æœè¾“å‡ºåˆ°å†…å­˜ä¸­çš„è¡¨ï¼Œæ¯æ¬¡è§¦å‘æ›´æ–°è¯¥è¡¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªè§¦å‘å™¨éƒ½åŸºäºä¸€ä¸ªå•ç‹¬çš„æ–‡ä»¶ï¼ˆè®¾ç½®çš„readé€‰é¡¹ï¼‰ã€‚Sparkå°†å¯¹å†…å­˜è¡¨ä¸­çš„æ•°æ®è¿›è¡Œæ›´æ–°ï¼Œè¿™æ ·å°†å§‹ç»ˆæ‹¥æœ‰ä¸Šä¸€æ¬¡èšåˆä¸­æŒ‡å®šçš„æœ€é«˜å€¼ï¼š

**Scala**

```scala
purchaseByCustomerPerHour.writeStream
    .format("memory") // memory = store in-memory table
    .queryName("customer_purchases") // the name of the in-memory table
    .outputMode("complete") // complete = all the counts should be in the table
    .start()
```

**Python**

```python
# in Python
purchaseByCustomerPerHour.writeStream\
    .format("memory")\
    .queryName("customer_purchases")\
    .outputMode("complete")\
    .start()
```

å½“å¯åŠ¨æµå¤„ç†æ—¶ï¼Œå¯ä»¥å¯¹å®ƒè¿è¡ŒæŸ¥è¯¢ï¼Œä»¥è°ƒè¯•å¦‚æœå°†å…¶å†™å…¥ç”Ÿäº§æ¥æ”¶å™¨ï¼Œç»“æœå°†æ˜¯ä»€ä¹ˆæ ·å­ï¼š

```
spark.sql("""
  SELECT *
  FROM customer_purchases
  ORDER BY `sum(total_cost)` DESC
  """)
  .show(5)
```

å°†å¤„ç†ç»“æœè¾“å‡ºåˆ°æ§åˆ¶å°ï¼š

```python
purchaseByCustomerPerHour.writeStream
    .format("console")
    .queryName("customer_purchases_2")
    .outputMode("complete")
    .start()
```

ä¸åº”è¯¥åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨è¿™ä¸¤ç§æµå¼æ•°æ®å¤„ç†æ–¹æ³•ã€‚æ³¨æ„window()æ˜¯å»ºç«‹åœ¨äº‹ä»¶æ—¶é—´ä¸Šçš„ï¼Œè€Œä¸æ˜¯Sparkå¤„ç†æ•°æ®çš„æ—¶é—´ã€‚è¿™æ˜¯ç»“æ„åŒ–æµå·²ç»è§£å†³çš„Sparkæµçš„ç¼ºç‚¹ä¹‹ä¸€ã€‚

## 3.3 æœºå™¨å­¦ä¹ ä¸é«˜çº§æ•°æ®åˆ†æ

Sparkçš„å¦ä¸€ä¸ªæµè¡Œæ–¹é¢æ˜¯å®ƒèƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ªå†…ç½®çš„æœºå™¨å­¦ä¹ ç®—æ³•åº“MLlibæ¥æ‰§è¡Œå¤§è§„æ¨¡çš„æœºå™¨å­¦ä¹ ã€‚MLlibå…è®¸å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€æ•°æ®æ¸…æ´—ã€æ•°æ®æ•´ç†ã€æ¨¡å‹è®­ç»ƒå’ŒæŒ‰æ¯”ä¾‹è¿›è¡Œé¢„æµ‹ã€‚ç”šè‡³å¯ä»¥ä½¿ç”¨åœ¨MLlibä¸­è®­ç»ƒçš„æ¨¡å‹åœ¨ç»“æ„åŒ–æµä¸­è¿›è¡Œé¢„æµ‹ã€‚Sparkæä¾›äº†ä¸€ä¸ªå¤æ‚çš„æœºå™¨å­¦ä¹ APIï¼Œç”¨äºæ‰§è¡Œå„ç§æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œä»åˆ†ç±»åˆ°å›å½’ï¼Œä»èšç±»åˆ°æ·±åº¦å­¦ä¹ ã€‚æœ¬èŠ‚ä¸­ï¼Œå°†ä½¿ç”¨K-Meansèšç±»ç®—æ³•è¿›è¡Œä¸€äº›æ¼”ç¤ºã€‚

**K-Meansèšç±»ç®—æ³•**

ğ˜¬-Meansæ˜¯ä¸€ç§èšç±»ç®—æ³•ï¼Œå…¶ä¸­Kä¸ªä¸­å¿ƒåœ¨æ•°æ®ä¸­éšæœºåˆ†é…ã€‚ç„¶åå°†æœ€æ¥è¿‘è¯¥ç‚¹çš„ç‚¹â€œæŒ‡å®šâ€ç»™ç±»ï¼Œå¹¶è®¡ç®—æŒ‡å®šç‚¹çš„ä¸­å¿ƒã€‚è¿™ä¸ªä¸­å¿ƒç‚¹å«åšè´¨å¿ƒã€‚ç„¶åï¼Œæ ‡è®°ç¦»è´¨å¿ƒæœ€è¿‘çš„ç‚¹ä¸ºè¯¥è´¨å¿ƒçš„ç±»ï¼Œå¹¶å°†è´¨å¿ƒç§»åˆ°è¯¥ç‚¹ç°‡çš„æ–°ä¸­å¿ƒã€‚åœ¨æœ‰é™çš„è¿­ä»£é›†é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ–è€…ç›´åˆ°æ”¶æ•›ï¼ˆä¸­å¿ƒç‚¹åœæ­¢æ”¹å˜ï¼‰ã€‚

MLlibä¸­çš„æœºå™¨å­¦ä¹ ç®—æ³•è¦æ±‚å°†æ•°æ®è¡¨ç¤ºä¸ºæ•°å€¼ã€‚å½“å‰çš„æ•°æ®ç”±å„ç§ä¸åŒçš„ç±»å‹è¡¨ç¤ºï¼ŒåŒ…æ‹¬æ—¶é—´æˆ³ã€æ•´æ•°å’Œå­—ç¬¦ä¸²ã€‚å› æ­¤éœ€è¦æŠŠè¿™äº›æ•°æ®è½¬æ¢æˆä¸€äº›æ•°å€¼è¡¨ç¤ºã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œå°†ä½¿ç”¨å‡ ä¸ªDataFrames transofrmtinæ¥æ“ä½œæ—¥æœŸæ•°æ®ï¼š

```python
staticDataFrame.printSchema()

root
 |-- InvoiceNo: string (nullable = true)
 |-- StockCode: string (nullable = true)
 |-- Description: string (nullable = true)
 |-- Quantity: integer (nullable = true)
 |-- InvoiceDate: timestamp (nullable = true)
 |-- UnitPrice: double (nullable = true)
 |-- CustomerID: double (nullable = true)
 |-- Country: string (nullable = true)
```

**Scala**

```scala
import org.apache.spark.sql.functions.date_format
val preppedDataFrame = staticDataFrame
  .na.fill(0)
  .withColumn("day_of_week", date_format($"InvoiceDate", "EEEE"))
  .coalesce(5)
```

**Python**

```python
from pyspark.sql.functions import date_format, col
preppedDataFrame = staticDataFrame\
  .na.fill(0)\
  .withColumn("day_of_week", date_format(col("InvoiceDate"), "EEEE"))\
  .coalesce(5)
```

è¿˜éœ€è¦å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œå°†åœ¨æŸä¸ªè´­ä¹°å‘ç”Ÿçš„æ—¥æœŸä¹‹å‰æ‰‹åŠ¨æ‰§è¡Œæ­¤æ“ä½œï¼›ä½†æ˜¯ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨MLlibçš„ transformation apié€šè¿‡TrainValidationSplitæˆ–CrossValidatoråˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆå°†åœ¨ç¬¬å…­éƒ¨åˆ†è¯¦ç»†ä»‹ç»ï¼‰ï¼š

**Scala**

```scala
val trainDataFrame = preppedDataFrame.where("InvoiceDate < '2011-07-01'")
val testDataFrame = preppedDataFrame.where("InvoiceDate >= '2011-07-01'")
```

**Python**

```python
trainDataFrame = preppedDataFrame.where("InvoiceDate < '2011-07-01'")
testDataFrame = preppedDataFrame.where("InvoiceDate >= '2011-07-01'")
```

K-Meansèšç±»çš„ä»£ç ç¤ºä¾‹ï¼š

**Scala**

```scala
import org.apache.spark.ml.feature.OneHotEncoder
import org.apache.spark.ml.feature.StringIndexer
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.clustering.KMeans


val encoder = new OneHotEncoder().setInputCol("day_of_week_index")
  .setOutputCol("day_of_week_encoded")

val vectorAssembler = new VectorAssembler()
  .setInputCols(Array("UnitPrice", "Quantity", "day_of_week_encoded"))
  .setOutputCol("features")

val transformationPipeline = new Pipeline()
  .setStages(Array(indexer, encoder, vectorAssembler))

val fittedPipeline = transformationPipeline.fit(trainDataFrame)

val transformedTraining = fittedPipeline.transform(trainDataFrame)

transformedTraining.cache()

val kmeans = new KMeans()
  .setK(20)
  .setSeed(1L)

val kmModel = kmeans.fit(transformedTraining)
kmModel.computeCost(transformedTraining)
val transformedTest = fittedPipeline.transform(testDataFrame)
kmModel.computeCost(transformedTest)
```

**Python**

```python
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.clustering import KMeans


indexer = StringIndexer().setInputCol("day_of_week").setOutputCol("day_of_week_index")

encoder = OneHotEncoder().setInputCol("day_of_week_index")
  .setOutputCol("day_of_week_encoded")

vectorAssembler = VectorAssembler().setInputCols(["UnitPrice", "Quantity", 		        "day_of_week_encoded"]).setOutputCol("features")

fittedPipeline = transformationPipeline.fit(trainDataFrame)
transformedTraining = fittedPipeline.transform(trainDataFrame)
kmModel = kmeans.fit(transformedTraining)
kmModel.computeCost(transformedTraining)
transformedTest = fittedPipeline.transform(testDataFrame)
kmModel.computeCost(transformedTest)
```

## 3.4 å¼¹æ€§æ•°æ®é›†(RDD)ï¼šSparkçš„ä½çº§API

SparkåŒ…å«è®¸å¤šä½çº§APIï¼Œå…è®¸é€šè¿‡å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(RDD)å¯¹ä»»æ„Javaå’ŒPythonå¯¹è±¡è¿›è¡Œæ“ä½œã€‚å®é™…ä¸Šï¼ŒSparkä¸­çš„æ‰€æœ‰å†…å®¹éƒ½æ„å»ºåœ¨RDDä¹‹ä¸Šã€‚DataFrameæ“ä½œæ„å»ºåœ¨RDDä¹‹ä¸Šï¼Œå¹¶å‘ä¸‹ç¼–è¯‘åˆ°è¿™äº›è¾ƒä½çº§åˆ«çš„å·¥å…·ï¼Œä»¥æ–¹ä¾¿å’Œéå¸¸é«˜æ•ˆçš„åˆ†å¸ƒå¼æ‰§è¡Œã€‚æœ‰ä¸€äº›ä¸œè¥¿å¯ä»¥ä½¿ç”¨rddï¼Œç‰¹åˆ«æ˜¯åœ¨è¯»å–æˆ–æ“ä½œåŸå§‹æ•°æ®æ—¶ï¼Œä½†æ˜¯åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ‚¨åº”è¯¥åšæŒä½¿ç”¨ç»“æ„åŒ–apiã€‚RDDçš„çº§åˆ«ä½äºæ•°æ®å¸§ï¼Œå› ä¸ºå®ƒä»¬å‘æœ€ç»ˆç”¨æˆ·æ˜¾ç¤ºç‰©ç†æ‰§è¡Œç‰¹æ€§ï¼ˆå¦‚åˆ†åŒºï¼‰ã€‚

rddåœ¨Scalaå’ŒPythonä¸­éƒ½æœ‰ã€‚ç”¨æˆ·ä¸åº”è¯¥ä¸ºäº†æ‰§è¡Œè®¸å¤šä»»åŠ¡è€Œå¤§é‡ä½¿ç”¨RDDï¼Œé™¤éç»´æŠ¤çš„æ˜¯æ—§çš„Sparkä»£ç ã€‚æ–°ç‰ˆSparkä¸­å¤„ç†ä¸€äº›éå¸¸åŸå§‹çš„æœªå¤„ç†å’Œéç»“æ„åŒ–æ•°æ®ä¹‹å¤–ï¼Œåº”è¯¥ä½¿ç”¨RDDè€Œä¸æ˜¯ç»“æ„åŒ–apiã€‚

## 3.5 Sparkçš„ç”Ÿæ€ç³»ç»Ÿå’Œè½¯ä»¶åŒ…

Sparkæœ€æ£’çš„éƒ¨åˆ†ä¹‹ä¸€æ˜¯ç¤¾åŒºåˆ›å»ºçš„åŒ…å’Œå·¥å…·ç”Ÿæ€ç³»ç»Ÿã€‚éšç€è¿™äº›å·¥å…·çš„æˆç†Ÿå’Œå¹¿æ³›åº”ç”¨ï¼Œå…¶ä¸­ä¸€äº›å·¥å…·ç”šè‡³è¿›å…¥äº†æ ¸å¿ƒSparké¡¹ç›®ã€‚æ•°é‡è¶…è¿‡300ä¸ªæˆ–æ›´å¤šçš„è½¯ä»¶åŒ…ç»å¸¸è¢«æ·»åŠ ã€‚å¯ä»¥åœ¨Spark-Packages.orgä¸Šæ‰¾åˆ°SparkåŒ…çš„æœ€å¤§ç´¢å¼•ï¼Œåœ¨é‚£é‡Œä»»ä½•ç”¨æˆ·éƒ½å¯ä»¥å‘å¸ƒåˆ°æ­¤åŒ…å­˜å‚¨åº“ã€‚è¿˜æœ‰è®¸å¤šå…¶ä»–çš„é¡¹ç›®å’ŒåŒ…å¯ä»¥æ‰¾åˆ°ï¼Œä¾‹å¦‚ï¼Œåœ¨GitHubä¸Šã€‚