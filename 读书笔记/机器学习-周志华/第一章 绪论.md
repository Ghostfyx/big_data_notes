机器学习是目前信息技术中最激动人心的方向之一，其应用已经深入到生活的各个层面且与普通人的日常生活密切相关。本文为清华大学最新出版的《机器学习》教材的Learning Notes，书作者是南京大学周志华教授，多个大陆首位彰显其学术奢华。本篇主要介绍了该教材第一章的知识点以及自己一点浅陋的理解。

# 1.1 引言

## 1.1.1 机器学习定义

机器学习致力于通过计算的手段，利用**经验**改善系统自身的性能。在计算机系统中，经验通常以数据的形式存储，因此，机器学习的主要研究内容是：关于在计算机上从数据中产生“模型”的算法，即学习算法（learning algorithm）。

另一本经典教材的作者Mitchell给出了一个形式化的定义，假设：

- P：计算机程序在某任务类T上的性能。
- T：计算机程序希望实现的任务类。
- E：表示经验，即历史的数据集。

若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。

# 1.2 基本术语

- 数据集：所有数据的集合，其中每一条数据是关于一个事件或者对象的描述，一般使用**D**表示，公式(1)表示包含i个样本的数据集。

$$
D = \{x_1, x_2, \dots ,x_i\}
$$

- 样本：数据集中的每一条数据或者实例，使用$x_i$表示

$$
x_i = \{x_{i1}, x_{i2},\dots ,x_{id}\} \tag2
$$

- 特征（feature）／属性（attribute）：反应样本在某方面表现或者性质的事项。如公式(2)所示$x_{ij}$表示样本i的第$j$维属性的特征值，$d$称为样本的维数。

- 特征空间（feature space）／样本空间：数据集中所有特征张成的空间。

$$
D = \left[
 \begin{matrix}
   x_{11} & x_{12} & \dots & x_{1d}\\
   x_{21} & x_{22} & \dots & x_{2d}  \\
   \vdots & \vdots & \ddots & \vdots  \\
   x_{m1} & x_{m2} & \dots & x_{md}
  \end{matrix} 
\right]
$$

- 维数（dimensionality）：一个样本的特征数

- 学习 （Learning）／训练（training）：从数据中学得模型的过程。

	计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”，即：学得模型$f$，对测试样本$x$，可得$y=f(x)$。定义：

- 训练集（trainning set）：所有训练样本的集合为，[特殊]。

- 测试集（test set）：所有测试样本的集合为，[一般]。

- 假设（hypothesis）：学得模型对应了关于数据的某种规律。

- 标记空间（label space）／输出空间：在监督学习和半监督学习中，对训练集数据的标记，$D=\{(x_1,y_1),(x_2, y_2), \dots, (x_i,y_i)\}，y \in Y$，$Y$  是所有标记的集合。
- 分类（classification）：标记空间为有限离散值。
- 回归（regression）：标记空间为连续实数空间，$|Y| > 2，y \in \mathbb{R}$。
- 聚类（clustering）：将训练集中的数据按照其分布分成若干组，每一组称为“簇（cluster）”，一般聚类均为无监督学习
- 泛化能力（generalization）：机器学习出来的模型适用于新样本的能力为，从特殊到一般。具有强泛化能力的模型能够适用于整个样本空间。
- 训练集与样本空间：训练集通常是样本空间的很小一部分，我们通常假设样本空间和训练集服从同一分布。

# 1.3 假设空间

归纳和演绎是科学推理的两大基本手段，前者是从特殊到一般的泛化，即从具体事实中归纳总结出一般规律；演绎是从一般到特殊的过程，即从基础原理推演出具体状况。

机器学习的目的是泛化，即通过对训练集中的数据进行归纳总结，得到一般规律，从而进一步对测试集的数据进行演绎推测。学习的过程可以看作是一个在所有假设空间中进行搜索的过程，搜素的目的是找到与训练集匹配的假设。

现实问题中我们通常会面临很大的假设空间，但学习过程是基于有限样本训练集进行的，因此，可能有多个假设与训练集一致，即存在一个与训练集一致的“假设集合”，我们称之为版本空间。

# 1.4 归纳偏好

通过学习得到的模型对应假设空间的一个假设，但是如果存在多个假设与训练集一致，但是在测试集中，**版本空间**中不同假设可能会有不同的输出，那么我们如何判断使用哪一个模型／假设呢？

对于一个具体的学习算法而言，它必须会产生一个模型，此时学习算法本身的偏好就会起到关键性作用，机器学习算法在学习过程中，对某种类型假设的偏好，称为“归纳偏好（inductive bias）”。归纳偏好可以看作是学习算法自身（某种原则）在庞大假设空间进行选择的启发式，奥卡姆剃刀是一种常用的基本原则。**奥卡姆剃刀原则**：若存在多个假设与观察一致，则选择最简单的一个。

事实上，归纳偏好对应了学习算法本身所作出的关旭“什么样的模型更好”的假设，使用中，算法的归纳偏好是否与问题本身匹配，大多时候直接决定了算法是否能够取得良好的性能。因此我们可以根基算法的偏好，选择适当的算法解决问题。

## 1.4.2 NFL定理

NFL定理(No Free Lunch Theorem)是没有免费午餐定理，在机器学习中，学习算法不可能解决所有问题换而言之，假设$\mathfrak{L}a$与$\mathfrak{L}b$，对于一个问题$\mathfrak{L}a$比$\mathfrak{L}b$好，则必然存在一些问题$\mathfrak{L}b$强于$\mathfrak{L}a$。
$$
\begin{aligned} 
\sum_{f}E_{ote}(\mathfrak{L}a\vert X,f) 
&= \sum_f\sum_h\sum{x\in\mathcal{X}-X}P(x)\mathbb{I}(h(x)\neq f(x))P(h\vert X,\mathfrak{L}a)\\
&=\sum{x\in\mathcal{X}-X}P(x) \sum_hP(h\vert X,\mathfrak{L}a)\sum_f\mathbb{I}(h(x)\neq f(x)) \\ 
&=\sum{x\in\mathcal{X}-X}P(x) \sum_hP(h\vert X,\mathfrak{L}a)\cfrac{1}{2}2^{\vert \mathcal{X} \vert} \\ 
&=\cfrac{1}{2}2^{\vert \mathcal{X} \vert}\sum{x\in\mathcal{X}-X}P(x) \sum_hP(h\vert X,\mathfrak{L}a) \\ 
&=2^{\vert \mathcal{X} \vert-1}\sum{x\in\mathcal{X}-X}P(x) \cdot 1\ 
\end{aligned}
$$
[解析]：第一步到第二步是因为$\sum_i^m\sum_j^n\sum_k^o a_ib_jc_k=\sum_i^m a_i \cdot \sum_j^n b_j \cdot \sum_k^o c_k$；第二步到第三步：首先要知道此时$f$的定义为**任何能将样本映射到{0,1}的函数+均匀分布**，也即不止一个$f$且每个$f$出现的概率相等，例如样本空间只有两个样本时：$ \mathcal{X}={x_1,x_2},\vert \mathcal{X} \vert=2$，那么所有的真实目标函数$f$为： $$\begin{aligned} f_1:f_1(x_1)=0,f_1(x_2)=0;\ f_2:f_2(x_1)=0,f_2(x_2)=1;\ f_3:f_3(x_1)=1,f_3(x_2)=0;\ f_4:f_4(x_1)=1,f_4(x_2)=1; \end{aligned}$$ 一共$2^{\vert \mathcal{X} \vert}=2^2=4$个真实目标函数。所以此时通过算法$\mathfrak{L}_a$学习出来的模型$h(x)$对每个样本无论预测值为0还是1必然有一半的$f$与之预测值相等，例如，现在学出来的模型$h(x)$对$x_1$的预测值为1，也即$h(x_1)=1$，那么有且只有$f_3$和$f_4$与$h(x)$的预测值相等，也就是有且只有一半的$f$与它预测值相等，所以$\sum_f\mathbb{I}(h(x)\neq f(x)) = \cfrac{1}{2}2^{\vert \mathcal{X} \vert} $；第三步一直到最后显然成立。值得一提的是，在这里我们定义真实的目标函数为**“任何能将样本映射到{0,1}的函数+均匀分布”**，但是实际情形并非如此，通常我们只认为能高度拟合已有样本数据的函数才是真实目标函数，例如，现在已有的样本数据为${(x_1,0),(x_2,1)}$，那么此时$f_2$才是我们认为的真实目标函数，由于没有收集到或者压根不存在${(x_1,0),(x_2,0)},{(x_1,1),(x_2,0)},{(x_1,1),(x_2,1)}$这类样本，所以$f_1,f_3,f_4$都不算是真实目标函数。