# 第一章 大数据分析

- 用数千个特征和数十亿交易来构建信用卡欺诈检测模型
- 向数百万用户智能地推荐数百万产品
- 通过模拟包含数百万金融工具的组合投资来评估金融风险
- 轻松地操作成千上万个人类基因的相关数据以发现致病极硬

大数据时代的信息规模是巨大的(参考大数据4V特点)。

## 1.1 数据科学面临的挑战

数据科学界有几个硬道理是不能违背的：

第一，成功的分析中，大部分工作是数据预处理。数据是混乱的，在让数据产生价值之前，必须对数据进行清洗、处理、融合、挖掘和许多其他操作，特别是在大数据集，由于人为很难检查，为了知道需要哪些预处理步骤，甚至需要采用计算方法。花在特征提取和选择上的时间比选择和实现算法的时间还要多。

第二，迭代是数据科学的基础之一。建模和分析经常需要对一个数据集进行多次遍历。这其中一方面是由机器学习算法和统计过程本身造成的，常用的优化过程，比如随机梯度下降和最大似然估计，收敛前都需要多次扫描输入数据。在构建模型时，往往很难第一次就得到理想的结果。选择正确的特征、挑选合适的算法、运行恰当的显著性测试，找到合适的超参数，这些工作都需要反复试验。框架每次访问数据都需要读磁盘，这样会增加延迟，降低探索数据的速度，限制了数据科学家进行试验的次数。

第三，构建完表现卓越的模型不等于大功告成。数据科学的目标在于让部署对不懂数据科学的人有用。把模型以许多回归权值的形式存成文本文件，放在数据科学家的计算机里，这样做根本没有实现数据科学的目标。数据推荐引擎和实时欺诈检测系统时最常见的数据应用，这些应用中，模型作为生产服务的一部分，需要定期甚至实时重建。

在这些场景下，有必要区别是实验环境下分析还是生产环境下分析。在实验环境下，数据科学家进行探索性分析，将数据图形化并用各种理论来测试，用各种特征做实验，用辅助数据源来赠钱数据，试验各种算法，希望从中找到一两个有效算法。在生产环境下，构建数据应用时，数据科学家进行操作式分析，把模型打包成服务，作为现实世界的决策依据。线性探索语言通常使用R之类的语言，在构建生产应用时，数据处理过程则完全用Java或C++重写。

如果用于建模的原始语言也可用于生产应用，则能节约开发成本。R之类的语言运行缓慢，很难将其与生产基础设施技术平台集成，而java和C++之类语言有很难用于探索性分析，它们缺乏交互式操作数据所需的REPL(read-evaluate-print-loop，读取-计算-打印-循环)环境，因此需要一种既能轻松建模又适合生产系统的框架。

## 1.2 认识Apache Spark

Spark是一个开源框架，作为计算引擎，它把程序分发到集群中的许多机器，同时提供了一个优雅的编程模型。

Spark的前辈Apache Hadoop的MapReduce革新了海量数据的计算方式，为运行在成百上千台机器上的并行程序提供了简单的编程模型。MapReduce引擎几乎可以做到线性扩展，随着数据量的增加，可以通过增加更多的计算机来保持作业时间不变。而且MapReduce是健壮的，将工作拆分为多个小任务，运行在数千台机器上，能优雅地处理失败的任务，并且不影响任务所属作业的正确执行。

Spark继承了MapReduce的线性扩展和容错性，同时做了一些重量级扩展：

- 摒弃了MapReduce先map再Reduce这样的严格方式，Spark引擎可以执行更通用的有向无环图(directed acyclic graph,DAG)算子，这意味着MapReduce中需要将中间计算结果写入分布式文件系统时，Spark能将中间结果直接传入流水线作业的下一步；
- Spark的Dataset和DataFrame抽象使开发人员将流水处理线上的任何点物化在跨集群节点的内存中，这样后续步骤如果需要相同数据集就不必重新计算或从磁盘中加载，这个特性使Spark可以应用于以前分布式处理引擎中无法胜任的场景中——非常适合大量迭代算法
- Spark契合了数据科学领域的硬道理，认识到构建数据应用的最大瓶颈不是CPU、磁盘或网络，而是分析人员的生产率。通过将于处理到模型评价的整个流水线整合在一个编程环境中，Spark大大加速了开发过程。Spark编程模型富有表达力，在REPL下包装了一组分析库，省去了多次往返IDE的开销。Spark还避免了采样和从Hadoop分布式文件系统数据多次读取的问题。

在数据处理和ETL方面，Spark的目标是成为大数据界的Python而不是大数据界的MATLAB。Spark的内存缓存使它适用于微观和宏观两个层面的迭代计算。

Spark还紧密集成Hadoop生态系统的许多工具。能够读写MapReduce支持的所有数据格式，可以与Hadoop上常用数据格式，如Apache Avro和Apache Parquet进行交互；能够读写NoSQL数据库，比如Apache HBase和Apache Cassandra。流式处理组件Spark Streaming能够连续从Apache Flume和Apache kafka之类的系统读取数据；SQL库能和Apache Hive Metastore交互，通过Hive on Spark，Spark能够代替MapReduce作为Hive底层的执行引擎；可以运行在Hadoop集群调度和资源管理YARN上，这样Spark可以和MapReduce及Apache Impala等其他处理引擎动态共享集群和管理策略。

## 1.3 关于本书

本书旨在帮助读者建立用Spark在大规模数据集上进行复杂分析的感觉，涉及数据清洗、数据预处理和数据探索，但不涉及模型的构建和评价，并会描述怎样将结果变为生产应用。在第二章之后，每一章都分别讨论了Spark在不同领域进行数据分析的实例，将涉及Scala、Spark、机器学习和数据分析。