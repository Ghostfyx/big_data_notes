# 第二章 用Scala和Spark进行数据分析

数据清洗是数据科学项目的第一步，往往也是最重要的一步。许多灵巧的分析最后功败垂成，原因就是分析的数据存在严重的质量问题，或者数据中某些因素使分析产生偏见，或数据科学家得出根本不存在的规律。

数据科学家最为人称道的是在数据分析生命周期的每一个阶段都能发现有意思的，有价值的问题。在一个分析项目的早期阶段，投入的技能和思考越多，对最终的产品就越有信心。

## 2.1 数据科学家的Scala

对数据处理和分析，数据科学家往往都有自己钟爱的工具，比如R或者Python。为了能在R或Python中直接使用Spark，Spark上开发了专门的类库和工具包。Spark框架是用Scala语言编写的，采用与底层架构相同的编程语言有诸多好处：

- 性能开销小

	 为了能在基于JVM语言(比如Scala)上运行用于R或Python编写的算法，必须在不同环境中传递代码和数据，这会付出代价，而且在转换过程中信息时有丢失。

- 能用上最新的版本和最好的功能

- 有助于了解Spark原理

使用Spark和Scala做数据分析则是一种完全不同的体验，因为可以选择相同的言语完成所有的事情。借助Spark，用Scala代码读取集群上的数据，把Scala代码发送到集群上完成相同的转换，可以使用Spark高级API，使用Spark SQL注册UDF。在同一个环境中完成所有数据处理和分析，不用考虑数据本身在何处存放和在何处处理。

## 2.2 Spark编程模型

Spark编程模型始于数据集，而数据集往往存放在分布式持久化存储上，比如HDFS。编写Spark程序通常包括一些列相关步骤：

（1）在输入数据集上定义一组转换

（2）调用action，可以将转换后的数据集保存到持久化存储上，或者把结果返回到驱动器程序的本地存储

（3）运行本地计算，处理分布式计算结果，本地计算有助于确定下一步转换和action

要想理解Spark，就必须理解Spark框架提供的两种抽象：存储和执行。Spark优美的搭配这两类抽象，可以将数据处理管道中的任何中间步骤缓在内存里已备后用。

## 2.3 记录关联问题

问题大致情况如下：我们有大量来自一个或多个源系统的记录，其中多种不同的记录可能代码相同的基础实体：比如客户、病人、业务地址或事件。每个实体有若干属性，比如姓名、地址、生日。我们需要根据这些属性找到那些代表相同实体的记录，不幸的是，有些属性值有问题：格式不一致，或有笔误，或信息缺失，如果简单地对这些属性作相等性测试，则会漏掉许多重复记录。举例如下，表2-1列出的几家商店的记录。

​													**表2-1 记录关联问题的难点**

| 名称                         | 地址                     | 城市           | 州         | 电话           |
| ---------------------------- | ------------------------ | -------------- | ---------- | -------------- |
| Josh's Coffe Shop            | 1234 Sunset Boulevard    | West Hollywood | CA         | (213)-555-1212 |
| Josh Coffe                   | 1234 Sunset Blvd West    | Hollywood      | CA         | 555-1212       |
| Coffee Chain #1234           | 1400 Sunset Blvd #2      | Hollywood      | CA         | 206-555-1212   |
| Coffee Chain Regional Office | 1400 Sunset Blvd Suite 2 | Hollywood      | California | 206-555-1212   |

表中前两行其实指同一家咖啡店，但由于数据录入错误，这两项看起来是在不同城市，相反表后两行其实是同一咖啡连锁店的不同业务部门，尽管有相同的地址。

这个例子清楚地说明了记录关联为什么很困难：即使两组记录看起来很相似，但针对每组中的条目，我们确定重复的标准不一样，这种区别我们人类很容易理解，计算机却很难了解。

## 2.4 小试牛刀：Spark Shell和SparkContext

Spark-shell是Scala语言的一个REPL环境，同时针对Spark做了一些扩展。SparkContext负责协调集群上Spark作业的执行。

RDD是Spark提供的最基本的抽象，代表分布在集群中多台机器上的对象集合。Spark有两种方式可以创建RDD：

- 使用SparkContext基于外部数据源创建RDD，外部数据源包括HDFS上的文件、通过JDBC访问数据库表或Spark shell创建的本地对象集合
- 在一个或多个已有的RDD上执行转换操作来创建RDD，这些转换操作包括记录过滤、对具有相同键值的记录做汇总，把多个RDD关联在一起等

## 2.5 把数据集从集群上获取到客户端

使用RDD的first()、take()等方法可以将数据返回到客户端。

书中其余部分见此书代码。