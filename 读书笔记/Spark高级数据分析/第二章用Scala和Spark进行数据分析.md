# 第二章 用Scala和Spark进行数据分析

数据清洗是数据科学项目的第一步，往往也是最重要的一步。许多灵巧的分析最后功败垂成，原因就是分析的数据存在严重的质量问题，或者数据中某些因素使分析产生偏见，或数据科学家得出根本不存在的规律。

数据科学家最为人称道的是在数据分析生命周期的每一个阶段都能发现有意思的，有价值的问题。在一个分析项目的早期阶段，投入的技能和思考越多，对最终的产品就越有信心。

## 2.1 数据科学家的Scala

对数据处理和分析，数据科学家往往都有自己钟爱的工具，比如R或者Python。为了能在R或Python中直接使用Spark，Spark上开发了专门的类库和工具包。Spark框架是用Scala语言编写的，采用与底层架构相同的编程语言有诸多好处：

- 性能开销小

	 为了能在基于JVM语言(比如Scala)上运行用于R或Python编写的算法，必须在不同环境中传递代码和数据，这会付出代价，而且在转换过程中信息时有丢失。

- 能用上最新的版本和最好的功能

- 有助于了解Spark原理

使用Spark和Scala做数据分析则是一种完全不同的体验，因为可以选择相同的言语完成所有的事情。借助Spark，用Scala代码读取集群上的数据，把Scala代码发送到集群上完成相同的转换，可以使用Spark高级API，使用Spark SQL注册UDF。在同一个环境中完成所有数据处理和分析，不用考虑数据本身在何处存放和在何处处理。

## 2.2 Spark编程模型

Spark编程模型始于数据集，而数据集往往存放在分布式持久化存储上，比如HDFS。编写Spark程序通常包括一些列相关步骤：

（1）在输入数据集上定义一组转换

（2）调用action，可以将转换后的数据集保存到持久化存储上，或者把结果返回到驱动器程序的本地存储

（3）运行本地计算，处理分布式计算结果，本地计算有助于确定下一步转换和action

要想理解Spark，就必须理解Spark框架提供的两种抽象：存储和执行。Spark优美的搭配这两类抽象，可以将数据处理管道中的任何中间步骤缓在内存里已备后用。

## 2.3 记录关联问题

问题大致情况如下：我们有大量来自一个或多个源系统的记录，其中多种不同的记录可能代码相同的基础实体：比如客户、病人、业务地址或事件。每个实体有若干属性，比如姓名、地址、生日。我们需要根据这些属性找到那些代表相同实体的记录，不幸的是，有些属性值有问题：格式不一致，或有笔误，或信息缺失，如果简单地对这些属性作相等性测试，则会漏掉许多重复记录。举例如下，表2-1列出的几家商店的记录。

​													**表2-1 记录关联问题的难点**

| 名称                         | 地址                     | 城市           | 州         | 电话           |
| ---------------------------- | ------------------------ | -------------- | ---------- | -------------- |
| Josh's Coffe Shop            | 1234 Sunset Boulevard    | West Hollywood | CA         | (213)-555-1212 |
| Josh Coffe                   | 1234 Sunset Blvd West    | Hollywood      | CA         | 555-1212       |
| Coffee Chain #1234           | 1400 Sunset Blvd #2      | Hollywood      | CA         | 206-555-1212   |
| Coffee Chain Regional Office | 1400 Sunset Blvd Suite 2 | Hollywood      | California | 206-555-1212   |

表中前两行其实指同一家咖啡店，但由于数据录入错误，这两项看起来是在不同城市，相反表后两行其实是同一咖啡连锁店的不同业务部门，尽管有相同的地址。

这个例子清楚地说明了记录关联为什么很困难：即使两组记录看起来很相似，但针对每组中的条目，我们确定重复的标准不一样，这种区别我们人类很容易理解，计算机却很难了解。

## 2.4 小试牛刀：Spark Shell和SparkContext

Spark-shell是Scala语言的一个REPL环境，同时针对Spark做了一些扩展。SparkContext负责协调集群上Spark作业的执行。

RDD是Spark提供的最基本的抽象，代表分布在集群中多台机器上的对象集合。Spark有两种方式可以创建RDD：

- 使用SparkContext基于外部数据源创建RDD，外部数据源包括HDFS上的文件、通过JDBC访问数据库表或Spark shell创建的本地对象集合
- 在一个或多个已有的RDD上执行转换操作来创建RDD，这些转换操作包括记录过滤、对具有相同键值的记录做汇总，把多个RDD关联在一起等

## 2.5 把数据集从集群上获取到客户端

使用RDD的first()、take()、collect()等方法可以将数据返回到客户端。

take()方法向客户端返回RDD的第一个元素，常用与对数据集做常规检查；collect()方法向客户端返回一个包含所有RDD内容的数组。take(n)方法向客户端返回指定数量的记录。

```scala
df.first()
df.collect()
df.take(n)
```

------

**动作**

创建RDD的操作并不会导致集群执行分布式计算。相反，RDD只是定义了作为计算过程中间步骤的逻辑数据集，只有调用RDD上的action(动作)时分布式计算才会执行。例如：count动作返回RDD中的记录个数：

```scala
rdd.count()
```

collect动作返回一个包含RDD中所有对象的Array(数组)：

```
rdd.collect()
```

动作不一定向本地进程返回结果，saveAsTextFile动作将RDD的内容保存到持久化存储(比如HDFS)上；

```scala
rdd.saveAsTextFile("hdfs://user/ds/mynumbers")
```

------

scala声明函数使用def关键字，必须为函数的参数指定类型，但是没必要指定函数的返回类型，原因在于Scala编译器能根据方法计算逻辑推断函数返回类型：

```scala
  def isHeader(line:String) = line.contains("id_1")
```

Scala也支持显式的指定返回类型，特别是在函数体很长，代码复杂并且包含多个return语句的情况，这时候，Scala编译器不一定能推断出函数的返回类型，为了函数代码可读性更好，也可以指明函数的返回类型。

```scala
def isHeader2(line:String):Boolean = {
    line.contains("id_1")
  }
```

Scala的匿名函数有点类似于Python的Lambda函数，为了减少函数输入，比如在匿名函数的定义中，为了定义匿名函数并给参数指定名称，只输入了字符`x =>`，Scala也允许使用下划线(_)表示匿名函数的参数：

```scala
head.filter(x => !isHeader(x))
head.filter(!isHeader(_))
```

## 2.6 把代码从客户端发送到集群

之前执行的代码都左右在head数组中的数据上，这些数据都在客户端机器上。现在，我们打算在Spark里把刚写好的代码应用关联到记录数据集RDD rawblocks，该数据集在集群上的记录有几百万条。用于在过滤集群上的语法和本地机器上的语法一样。这正是Spark的强大之处。

它意味着我们可以先从集群中采样得到小的数据集，在小数据集上开发和调试数据处理代码，等一切就绪后再把代码发送到集群上处理完整的数据集就可以了。

## 2.7 从RDD到DataFrame

我们遇到的大部分数据集都有着合理的结构，要么因为它本来就是如此，要么因为已经有人已经对数据做好了清洗和结构化，我们没必要对花费精力自己写一套代码解析，只要简单地调用现成的类库，并利用数据的结构，即可解析成所需的结构，Spark 1.3引入了一个这样的数据结构——DataFrame。

DataFrame是一个建立在RDD之上的Spark抽象，专门为结构规整的数据集而设计，DataFrame的一条记录就是一行，每一行由若干个列组成，每一列的数据类型都有严格的定义，可以把DataFrame类型实例理解为Spark版本的关系数据库表。DataFrame这个名字可能会联想到R语言的data.frame对象，或者Python的pandas.DataFrame对象，但是Spark的DataFrame与它们有很大的不同，因为Spark的DataFrame对象代表一个分布式数据集，而不是所有数据都存储在同一台机器上的本地数据。

要为记录关联数据集创建一个DataFrame，需要用到SparkSession对象，SparkSession是SparkContext对象的一个封装，可以通过SparkSession直接访问到SparkContext：

```scala
sparkSession.sparkContext()
```

