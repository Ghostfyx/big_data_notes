# ç¬¬ä¸‰ç«  Sparkå·¥å…·é›†ä»‹ç»

ç¬¬äºŒç« ä»‹ç»äº†Sparkçš„æ ¸å¿ƒæ¦‚å¿µï¼ŒåŒ…æ‹¬Sparkç»“æ„åŒ–APIçš„transformationå’Œactionæ“ä½œã€‚è¿™äº›ç®€å•çš„æ¦‚å¿µæ˜¯å­¦ä¹ Apache Sparkç”Ÿæ€ç³»ç»Ÿä¸­ä¼—å¤šå·¥å…·åº“çš„åŸºç¡€ï¼Œå¦‚å›¾3-1æ‰€ç¤ºï¼ŒSparké™¤äº†æä¾›è¿™äº›ä½çº§APIå’Œç»“æ„åŒ–APIï¼Œè¿˜åŒ…æ‹¬ä¸€ç³»åˆ—æ ‡å‡†åº“æ¥æä¾›é¢å¤–çš„åŠŸèƒ½ã€‚

![](./img/3-1.jpg)

â€‹															**å›¾3-1 Spark APiä¸æ ‡å‡†ç±»åº“**

Sparkåº“æ”¯æŒå„ç§ä¸åŒçš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åˆ†æã€æœºå™¨å­¦ä¹ ã€æµå¤„ç†ï¼Œä»¥åŠæä¾›äºå…¶ä»–è®¡ç®—ç³»ç»Ÿå’Œå­˜å‚¨ç³»ç»Ÿè¿›è¡Œé›†æˆçš„èƒ½åŠ›ç­‰ã€‚æœ¬ç« ä»‹ç»äº†Sparkæ‰€æä¾›çš„å¤§éƒ¨åˆ†åŠŸèƒ½ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å†…å®¹: 

- ä½¿ç”¨Spark-submitè¿è¡Œåº”ç”¨ç¨‹åº
- Datasetï¼šç±»å‹å®‰å…¨çš„ç»“æ„åŒ–æ•°æ®API
- ç»“æ„åŒ–æµå¤„ç†
- æœºå™¨å­¦ä¹ ä¸é«˜çº§åˆ†æ
- å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(RDD)ï¼šSparkçš„ä½çº§API
- SparkR
- ç¬¬ä¸‰æ–¹è½¯ä»¶åŒ…ç”Ÿæ€ç³»ç»Ÿ

## 3.1 è¿è¡Œç”Ÿäº§åº”ç”¨ç¨‹åº

Sparkç®€åŒ–äº†å¼€å‘å’Œæ„å»ºå¤§æ•°æ®å¤„ç†ç¨‹åºçš„è¿‡ç¨‹ï¼ŒSparkè¿˜å¯ä»¥é€šè¿‡å†…ç½®çš„å‘½ä»¤è¡Œå·¥å…·spark-submitè½»æ¾åœ°å°†æµ‹è¯•çº§åˆ«çš„äº¤äº’å¼ç¨‹åºè½¬åŒ–ä¸ºç”Ÿäº§çº§åˆ«çš„åº”ç”¨ç¨‹åºã€‚Spark-submitå°†åº”ç”¨ç¨‹åºä»£ç å‘é€åˆ°é›†ç¾¤å¹¶å¯åŠ¨æ‰§è¡Œï¼Œåº”ç”¨ç¨‹åºå°†ä¸€è‡´è¿è¡Œï¼Œç›´åˆ°å®ƒé€€å‡º(å®Œæˆä»»åŠ¡)æˆ–é‡åˆ°é”™è¯¯ã€‚ç¨‹åºå¯ä»¥åœ¨Sparkæ‰€æœ‰æ”¯æŒé›†ç¾¤ç®¡ç†å™¨(standaloneã€Mesoså’Œyarn)ä¸‹è¿è¡Œã€‚ 

spark-submitæä¾›äº†å‡ ä¸ªæ§åˆ¶é¡¹ï¼Œå¯ä»¥æŒ‡å®šåº”ç”¨ç¨‹åºéœ€è¦çš„èµ„æºï¼Œä»¥åŠåº”ç”¨ç¨‹åºçš„è¿è¡Œæ–¹å¼å’Œè¿è¡Œå‚æ•°ç­‰ã€‚

å¯ä»¥ä½¿ç”¨Sparkæ”¯æŒçš„ä»»ä½•è¯­è¨€ä¸­ç¼–å†™åº”ç”¨ç¨‹åºï¼Œç„¶åæäº¤æ‰§è¡Œã€‚æœ€ç®€å•çš„ç¤ºä¾‹æ˜¯åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œåº”ç”¨ç¨‹åºï¼š

```sh
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local \
  ./examples/jars/spark-examples_2.11-2.2.0.jar 10
```

--classæŒ‡å®šè¦è¿è¡Œçš„JARåŒ…å’Œå¯åŠ¨ç±»

è¿è¡ŒPythonï¼š

```sh
./bin/spark-submit \
  --master local \
  ./examples/src/main/python/pi.py 10
```

é€šè¿‡æ›´æ”¹Spark-submitçš„masterå‚æ•°ï¼Œå¯ä»¥å°†åº”ç”¨ç¨‹åºæäº¤åˆ°Sparkçš„standaloneé›†ç¾¤èµ„æºç®¡ç†å™¨ã€Mesosæˆ–Yarnã€‚

## 3.2 Datasetï¼šç±»å‹å®‰å…¨çš„ç»“æ„åŒ–API

Datesetsæ˜¯Sparkç»“æ„åŒ–APIçš„ç±»å‹å®‰å…¨ç‰ˆæœ¬ã€‚ç”¨äºåœ¨Javaå’ŒScalaä¸­ç¼–å†™é™æ€ç±»å‹çš„ä»£ç ã€‚Dataset APIåœ¨Pythonå’ŒRä¸­ä¸å¯ç”¨ï¼Œå› ä¸ºè¿™äº›è¯­è¨€æ˜¯åŠ¨æ€ç±»å‹çš„ã€‚

å‰ä¸€ç« èŠ‚å¥½çš„DataFrameæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼çš„ç±»å‹ä¸ºRowçš„å¯¹è±¡é›†åˆï¼Œå®ƒå¯ä»¥å­˜å‚¨å¤šç§ç±»å‹çš„è¡¨æ ¼æ•°æ®ã€‚Dataset APIè®©ç”¨æˆ·å¯ä»¥ç”¨Java/Scalaç±»å®šä¹‰DataFrameä¸­çš„æ¯æ¡è®°å½•ï¼Œå¹¶å°†å…¶ä½œä¸ºç±»å‹å¯¹è±¡çš„é›†åˆæ¥æ“ä½œï¼Œç±»ä¼¼äºJava ArrayListæˆ–Scala Seqã€‚Datasetä¸­å¯ç”¨APIæ˜¯ç±»å‹å®‰å…¨çš„ï¼Œè¿™æ„å‘³ç€Datasetä¸­çš„å¯¹è±¡ä¸ä¼šè¢«è§†ä¸ºä¸å‡ºäº‹å®šä¹‰çš„ç±»ä¸åŒçš„å¦ä¸€ä¸ªç±»ã€‚è¿™ä½¿å¾—Datasetåœ¨ç¼–å†™å¤§å‹åº”ç”¨ç¨‹åºæ—¶å°¤å…¶æœ‰æ•ˆã€‚è¿™æ ·çš„è¯å¤šä¸ªè½¯ä»¶å·¥ç¨‹å¸ˆå¯ä»¥é€šè¿‡åå•†å¥½çš„æ¥å£è¿›è¡Œäº¤äº’ã€‚

Datasetç±»é€šè¿‡å†…éƒ¨åŒ…å«çš„å¯¹è±¡ç±»å‹è¿›è¡Œå‚æ•°åŒ–ï¼Œå¦‚Javaä¸­çš„`Dataset<T>`å’ŒScalaä¸­çš„`Dataset[T]`å°†ä»…åŒ…å«Tç±»çš„å¯¹è±¡ã€‚ä»ç±»å‹éœ€è¦Spark 2.0å¼€å§‹ï¼Œå—æ”¯æŒçš„ç±»å‹éµå¾ªJvaçš„Beanæ¨¡å¼ï¼Œæˆ–æ˜¯Scalaçš„caseç±»ã€‚ä¹‹æ‰€ä»¥è¿™äº›ç±»å‹éœ€è¦å—åˆ°é™åˆ¶ï¼Œæ˜¯å› ä¸ºSparkè¦èƒ½å¤Ÿè‡ªåŠ¨åˆ†æç±»å‹Tï¼Œå¹¶æœªDatasetä¸­çš„è¡¨æ ¼æ•°æ®åˆ›å»ºé€‚å½“çš„æ¨¡å¼ã€‚

Datasetå¦ä¸€ä¼˜ç‚¹æ˜¯ï¼šåªæœ‰åœ¨éœ€è¦æˆ–æƒ³è¦æ—¶æ‰èƒ½ä½¿ç”¨å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œå°†å®šä¹‰è‡ªå·±çš„æ•°æ®ç±»å‹ï¼Œå¹¶é€šè¿‡ä»»æ„mapå’Œfilterå‡½æ•°å¯¹å…¶è¿›è¡Œæ“ä½œã€‚åœ¨å®Œæˆæ“ä½œä¹‹åï¼ŒSparkå¯ä»¥è‡ªåŠ¨åœ°å°†å®ƒè½¬æ¢ä¸ºDataFrameï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ä½¿ç”¨SparkåŒ…å«çš„æ•°ç™¾ä¸ªå‡½æ•°æ¥è¿›ä¸€æ­¥æ“ä½œå®ƒã€‚è¿™ä½¿å¾—å®ƒå¾ˆå®¹æ˜“ä¸‹é™åˆ°è¾ƒä½çš„çº§åˆ«ï¼Œåœ¨å¿…è¦æ—¶æ‰§è¡Œç±»å‹å®‰å…¨ç¼–ç ï¼›ä¹Ÿå¯ä»¥å‡çº§åˆ°æ›´é«˜çº§çš„SQLè¿›è¡Œæ›´å¿«é€Ÿçš„åˆ†æã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå°ç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ç±»å‹å®‰å…¨å‡½æ•°å’ŒDataFrameç±»SQLè¡¨è¾¾å¼æ¥å¿«é€Ÿç¼–å†™ä¸šåŠ¡é€»è¾‘ï¼š

```scala
case class Flight(DEST_COUNTRY_NAME: String,
                  ORIGIN_COUNTRY_NAME: String,
                  count: BigInt)
val flightsDF = spark.read
  .parquet("/data/flight-data/parquet/2010-summary.parquet/")
val flights = flightsDF.as[Flight]
```

æœ€åä¸€ä¸ªä¼˜ç‚¹æ˜¯ï¼Œå½“åœ¨Datasetä¸Šè°ƒç”¨collectæˆ–takeæ—¶ï¼Œå®ƒå°†ä¼šæ‰‹æœºDatasetä¸­åˆé€‚ç±»å‹çš„å¯¹è±¡ï¼Œè€Œä¸æ˜¯DataFrameçš„Rowå¯¹è±¡ã€‚è¿™æ ·å¾ˆå®¹æ˜“åœ°ä¿è¯ç±»å‹å®‰å…¨ï¼Œå¹¶ä»¥åˆ†å¸ƒå¼å’Œæœ¬åœ°æ–¹å¼å®‰å…¨åœ°æ‰§è¡Œæ“ä½œï¼Œè€Œæ— éœ€æ›´æ”¹ä»£ç ï¼š

```scala
flights
  .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(flight_row => flight_row)
  .take(5)

flights
  .take(5)
  .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(fr => Flight(fr.DEST_COUNTRY_NAME, fr.ORIGIN_COUNTRY_NAME, fr.count + 5))
```

## 3.3 Structured Streaming

Structured Streamingæ˜¯ç”¨äºæµå¤„ç†çš„é«˜çº§APIï¼Œé€‚ç”¨äºSpark 2.2ä¹‹åçš„ç‰ˆæœ¬ã€‚å¯ä»¥åƒåœ¨æ‰¹å¤„ç†æ¨¡å¼ä¸‹ä¸€æ ·ï¼Œä½¿ç”¨Sparkçš„ç»“æ„åŒ–APIæ‰§è¡Œç»“æ„åŒ–æµå¤„ç†ï¼Œå¹¶ä»¥æµå¼æ–¹å¼è¿è¡Œå®ƒä»¬ï¼Œä½¿ç”¨ç»“æ„åŒ–æµå¤„ç†å¯ä»¥å‡å°‘å»¶è¿Ÿå¹¶å…è®¸å¢é‡å¤„ç†ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œå®ƒå¯ä»¥è®©ä½ å¿«é€Ÿåœ°ä»æµå¼ç³»ç»Ÿä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œè€Œå‡ ä¹ä¸éœ€è¦æ›´æ”¹ä»£ç ã€‚å¯ä»¥æŒ‰ç…§ä¼ ç»Ÿæ‰¹å¤„ç†ä½œä¸šçš„æ¨¡å¼è¿›è¡Œè®¾è®¡ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºæµå¼ä½œä¸šï¼Œå³å¢é‡å¤„ç†æ•°æ®ã€‚

çœ‹ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œè¯´æ˜å¼€å§‹ä½¿ç”¨ç»“æ„åŒ–æµæ˜¯å¤šä¹ˆå®¹æ˜“ã€‚å°†ä½¿ç”¨ä¸€ä¸ªé›¶å”®æ•°æ®é›†(https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/retail-data)ï¼Œè¯¥æ•°æ®é›†æœ‰ç‰¹å®šçš„æ—¥æœŸå’Œæ—¶é—´ä¾›æˆ‘ä»¬ä½¿ç”¨ã€‚å°†ä½¿ç”¨æŒ‰å¤©åˆ†ç»„çš„æ–‡ä»¶ï¼Œå…¶ä¸­ä¸€ä¸ªæ–‡ä»¶è¡¨ç¤ºä¸€å¤©çš„æ•°æ®ã€‚

é¦–å…ˆå°†å®ƒæ”¾åœ¨è¿™ç§æ ¼å¼ä¸­ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒè¿›ç¨‹ä»¥ä¸€è‡´å’Œè§„åˆ™çš„æ–¹å¼ç”Ÿæˆçš„æ•°æ®ï¼Œå¹¶å‘é€åˆ°ä¸€ä¸ªä½ç½®ï¼Œåœ¨é‚£é‡Œä½¿ç”¨Structured Streamingä½œä¸šå°†è¯»å–è¿™äº›æ•°æ®ã€‚

```csv
InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country
536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,2010-12-01 08:26:00,2.55,17...
536365,71053,WHITE METAL LANTERN,6,2010-12-01 08:26:00,3.39,17850.0,United Kin...
536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,2010-12-01 08:26:00,2.75,17850...
```

é¦–å…ˆæŒ‰ç…§é™æ€æ•°æ®é›†çš„å¤„ç†æ–¹æ³•æ¥è¿›è¡Œåˆ†æï¼Œå¹¶åˆ›å»ºä¸€ä¸ªDataFrameæ¥æ‰§è¡Œæ­¤æ“ä½œï¼Œè¿˜å°†ä»è¿™ä¸ªé™æ€æ•°æ®é›†åˆ›å»ºä¸€ä¸ªSchemaæ¨¡å¼ã€‚

**Scalaå®ç°**

```scala
val staticDataFrame = spark.read.format("csv")
  .option("header", "true")
  .option("inferSchema", "true")
  .load("/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
val staticSchema = staticDataFrame.schema
```

**Pythonå®ç°**

```python
staticDataFrame = spark.read.format("csv")\
  .option("header", "true")\
  .option("inferSchema", "true")\
  .load("/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
staticSchema = staticDataFrame.schema
```

ç”±äºä½¿ç”¨çš„æ˜¯æ—¶é—´åºåˆ—æ•°æ®ï¼Œæ‰€ä»¥åœ¨æ­¤ä¹‹å‰éœ€è¦å¼ºè°ƒå¦‚ä½•å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„å’Œèšåˆæ“ä½œã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œå°†æŸ¥çœ‹ç»™å®šå®¢æˆ·(CustomerId)è¿›è¡Œå¤§é‡é‡‡è´­çš„æ—¶é—´ã€‚ä¾‹å¦‚ï¼Œæ·»åŠ ä¸€ä¸ªtotal coståˆ—ï¼Œçœ‹çœ‹å®¢æˆ·èŠ±è´¹æœ€å¤šçš„é‚£ä¸ªæ—¥æœŸã€‚

`window()`å‡½æ•°ç†æ—¥æœŸå’Œæ—¶é—´æˆ³çš„æœ‰ç”¨å·¥å…·ï¼ŒåŒ…å«èšåˆä¸­æ¯å¤©çš„æ‰€æœ‰æ•°æ®ã€‚æ˜¯æ•°æ®ä¸­æ—¶é—´åºåˆ—çš„ä¸€ä¸ªçª—å£ã€‚ä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š

**Scala**

```scala
import org.apache.spark.sql.functions.{window, column, desc, col}
staticDataFrame
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))
  .sum("total_cost")
  .show(5)
```

**Python**

```python
from pyspark.sql.functions import window, column, desc, col
staticDataFrame\
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")\
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
  .sum("total_cost")\
  .show(5)
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```
+----------+--------------------+------------------+
|CustomerId|              window|   sum(total_cost)|
+----------+--------------------+------------------+
|   17450.0|[2011-09-20 00:00...|          71601.44|
...
|      null|[2011-12-08 00:00...|31975.590000000007|
+----------+--------------------+------------------+
```

æµå¼å¤„ç†ä»£ç ä¸æ‰¹å¤„ç†ä»£ç å®é™…ä¸Šå‡ ä¹æ²¡æœ‰ä»€ä¹ˆå˜åŒ–ã€‚æœ€å¤§çš„å˜åŒ–æ˜¯ä½¿ç”¨readStreamè€Œä¸æ˜¯readï¼Œå¦å¤–maxFilesPerTriggeré€‰é¡¹ï¼ŒæŒ‡å®šåº”è¯¥ä¸€æ¬¡è¯»å…¥çš„æ–‡ä»¶æ•°ã€‚è¿™æ˜¯ä¸ºäº†æ›´å¥½æ¼”ç¤ºæ•°æ®æµï¼Œåœ¨ç”Ÿäº§åœºæ™¯ä¸­ï¼Œè¿™å¯èƒ½ä¼šè¢«å¿½ç•¥ã€‚

**Scala**

```scala
val streamingDataFrame = spark.readStream
    .schema(staticSchema)
    .option("maxFilesPerTrigger", 1)
    .format("csv")
    .option("header", "true")
    .load("/data/retail-data/by-day/*.csv")
```

**Python**

```python
streamingDataFrame = spark.readStream
    .schema(staticSchema)
    .option("maxFilesPerTrigger", 1)
    .format("csv")
    .option("header", "true")
    .load("/data/retail-data/by-day/*.csv")
```

å¯¹æµæ•°æ®æ‰§è¡Œä¸ä¹‹å‰å¯¹é™æ€DataFrameä¸€æ ·çš„ä¸šåŠ¡é€»è¾‘(æŒ‰ç…§æ—¶é—´çª—å£ç»Ÿè®¡èŠ±è´¹)ï¼Œæµå¼æ•°æ®çš„æ‰¹å¤„ç†é€»è¾‘ï¼š

**scala**

```scala
val purchaseByCustomerPerHour = streamingDataFrame
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")
  .groupBy(
    $"CustomerId", window($"InvoiceDate", "1 day"))
  .sum("total_cost")
```

**Python**

```python
purchaseByCustomerPerHour = streamingDataFrame\
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")\
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
  .sum("total_cost")
```

è¿™ä»ç„¶æ˜¯ä¸€ä¸ªæƒ°æ€§æ“ä½œï¼Œå› æ­¤éœ€è¦è°ƒç”¨æµæ“ä½œæ¥å¼€å§‹æ‰§è¡Œæ­¤æ•°æ®æµã€‚

æµå¼å¤„ç†ä¸æ‰¹å¤„ç†çš„actionæ“ä½œæœ‰æ‰€ä¸åŒï¼Œå› ä¸ºé¦–å…ˆè¦å°†æµæ•°æ®ç¼“å­˜åˆ°æŸä¸ªåœ°æ–¹ï¼Œè€Œä¸æ˜¯åƒå¯¹é™æ€æ•°æ®é‚£æ ·ç›´æ¥è°ƒç”¨countå‡½æ•°(å¯¹æµæ•°æ®æ²¡æœ‰ä»»ä½•æ„ä¹‰)ã€‚æµæ•°æ®å°†è¢«ç¼“å­˜åˆ°ä¸€ä¸ªå†…å­˜ä¸Šçš„æ•°æ®è¡¨é‡Œã€‚åœ¨æ¯æ¬¡è¢«è§¦å‘å™¨è§¦å‘åæ›´æ–°è¿™ä¸ªå†…å­˜ç¼“å­˜ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä¹‹å‰è®¾ç½®çš„maxFilesPerTiggeré€‰é¡¹æ¯æ¬¡è¯»å®Œä¸€ä¸ªæ–‡ä»¶åéƒ½ä¼šè¢«è§¦å‘ï¼ŒSparkå°†åŸºäºæ–°è¯»å…¥çš„æ–‡ä»¶æ›´æ–°å†…å­˜æ•°æ®è¡¨çš„å†…å®¹ï¼Œè¿™æ ·çš„è¯ï¼Œèšåˆæ“ä½œå¯ä»¥ä½¿ç”¨ç»´æŠ¤ç€å†å²æ•°æ®ä¸­çš„æœ€å¤§å€¼ã€‚

**Scala**

```scala
purchaseByCustomerPerHour.writeStream
    .format("memory") // memoryä»£è¡¨å°†è¡¨å­˜å‚¨å†…å­˜
    .queryName("customer_purchases") //å­˜å…¥å†…å­˜çš„è¡¨å
    .outputMode("complete") // completeè¡¨ç¤ºä¿å­˜è¡¨ä¸­æ‰€æœ‰è®°å½•
    .start()
```

**Python**

```python
# in Python
purchaseByCustomerPerHour.writeStream\
    .format("memory")\
    .queryName("customer_purchases")\
    .outputMode("complete")\
    .start()
```

å½“å¯åŠ¨æµå¤„ç†æ—¶ï¼Œå¯ä»¥è¿è¡ŒæŸ¥è¯¢æ¥è°ƒè¯•è®¡ç®—ç»“æ„ï¼ŒæŸ¥çœ‹æˆ‘ä»¬çš„è®¡ç®—ç»“æœæ˜¯å¦å·²ç»è¢«å†™å…¥ç»“æœæ¥æ”¶å™¨ã€‚

```
spark.sql("""
  SELECT *
  FROM customer_purchases
  ORDER BY `sum(total_cost)` DESC
  """)
  .show(5)
```

ä¼šæ³¨æ„åˆ°ï¼Œè¾“å‡ºè¡¨æ ¼çš„å†…å®¹ä¼šéšç€è¯»å…¥æ›´å¤šçš„æ•°æ®è€Œå‘ç”Ÿå®æ—¶å˜åŒ–ã€‚åœ¨å¤„ç†å®Œæ¯ä¸ªæ–‡ä»¶åï¼Œç»“æœå¯èƒ½ä¼šæ ¹æ® æ•°æ®å‘ç”Ÿæ”¹å˜ï¼Œä¹Ÿå¯èƒ½ä¸ä¼šã€‚å› ä¸ºè¦æ ¹æ®å®¢æˆ·è´­ä¹°èƒ½åŠ›å¯¹å®¢æˆ·è¿›è¡Œåˆ†ç»„ã€‚å°†å¤„ç†ç»“æœè¾“å‡ºåˆ°æ§åˆ¶å°ï¼š

```python
purchaseByCustomerPerHour.writeStream
    .format("console")
    .queryName("customer_purchases_2")
    .outputMode("complete")
    .start()
```

ä¸åº”è¯¥åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨è¿™ä¸¤ç§æµå¼æ•°æ®å¤„ç†æ–¹æ³•ã€‚æ³¨æ„è¿™ä¸ªæ—¶é—´çª—å£æ˜¯åŸºäºäº‹ä»¶äº‹ä»¶çš„ï¼Œè€Œä¸æ˜¯Sparkå¤„ç†æ•°æ®çš„æ—¶é—´ï¼Œåœ¨æ–°çš„ç»“æ„åŒ–æµå¤„ç†è§£å†³è¿™ä¸ªé—®é¢˜ä»¥å‰ï¼Œè¿™ä¸ªæ˜¯Sparkç¼ºç‚¹ä¹‹ä¸€ã€‚

## 3.3 æœºå™¨å­¦ä¹ ä¸é«˜çº§æ•°æ®åˆ†æ

Sparkçš„å¦ä¸€ä¸ªæµè¡Œæ–¹é¢æ˜¯å®ƒèƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ªå†…ç½®çš„æœºå™¨å­¦ä¹ ç®—æ³•åº“MLlibæ¥æ‰§è¡Œå¤§è§„æ¨¡çš„æœºå™¨å­¦ä¹ ã€‚MLlibå…è®¸å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€æ•°æ®æ¸…æ´—ã€æ•°æ®æ•´ç†ã€æ¨¡å‹è®­ç»ƒå’ŒæŒ‰æ¯”ä¾‹è¿›è¡Œé¢„æµ‹ã€‚ç”šè‡³å¯ä»¥ä½¿ç”¨åœ¨MLlibä¸­è®­ç»ƒçš„æ¨¡å‹åœ¨ç»“æ„åŒ–æµä¸­è¿›è¡Œé¢„æµ‹ã€‚Sparkæä¾›äº†ä¸€ä¸ªå¤æ‚çš„æœºå™¨å­¦ä¹ APIï¼Œç”¨äºæ‰§è¡Œå„ç§æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œä»åˆ†ç±»åˆ°å›å½’ï¼Œä»èšç±»åˆ°æ·±åº¦å­¦ä¹ ã€‚æœ¬èŠ‚ä¸­ï¼Œå°†ä½¿ç”¨K-Meansèšç±»ç®—æ³•è¿›è¡Œä¸€äº›æ¼”ç¤ºã€‚

**K-Meansèšç±»ç®—æ³•**

ğ˜¬-Meansæ˜¯ä¸€ç§èšç±»ç®—æ³•ï¼Œå…¶ä¸­Kä¸ªä¸­å¿ƒåœ¨æ•°æ®ä¸­éšæœºåˆ†é…ã€‚ç„¶åå°†æœ€æ¥è¿‘è¯¥ç‚¹çš„ç‚¹â€œæŒ‡å®šâ€ç»™ç±»ï¼Œå¹¶è®¡ç®—æŒ‡å®šç‚¹çš„ä¸­å¿ƒã€‚è¿™ä¸ªä¸­å¿ƒç‚¹å«åšè´¨å¿ƒã€‚ç„¶åï¼Œæ ‡è®°ç¦»è´¨å¿ƒæœ€è¿‘çš„ç‚¹ä¸ºè¯¥è´¨å¿ƒçš„ç±»ï¼Œå¹¶å°†è´¨å¿ƒç§»åˆ°è¯¥ç‚¹ç°‡çš„æ–°ä¸­å¿ƒã€‚åœ¨æœ‰é™çš„è¿­ä»£é›†é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ–è€…ç›´åˆ°æ”¶æ•›ï¼ˆä¸­å¿ƒç‚¹åœæ­¢æ”¹å˜ï¼‰ã€‚

MLlibä¸­çš„æœºå™¨å­¦ä¹ ç®—æ³•è¦æ±‚å°†æ•°æ®è¡¨ç¤ºä¸ºæ•°å€¼ã€‚å½“å‰çš„æ•°æ®ç”±å„ç§ä¸åŒçš„ç±»å‹è¡¨ç¤ºï¼ŒåŒ…æ‹¬æ—¶é—´æˆ³ã€æ•´æ•°å’Œå­—ç¬¦ä¸²ã€‚å› æ­¤éœ€è¦æŠŠè¿™äº›æ•°æ®è½¬æ¢æˆä¸€äº›æ•°å€¼è¡¨ç¤ºã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œå°†ä½¿ç”¨å‡ ä¸ªDataFrames transofrmtionæ¥æ“ä½œæ—¥æœŸæ•°æ®ï¼š

```python
staticDataFrame.printSchema()

root
 |-- InvoiceNo: string (nullable = true)
 |-- StockCode: string (nullable = true)
 |-- Description: string (nullable = true)
 |-- Quantity: integer (nullable = true)
 |-- InvoiceDate: timestamp (nullable = true)
 |-- UnitPrice: double (nullable = true)
 |-- CustomerID: double (nullable = true)
 |-- Country: string (nullable = true)
```

**Scala**

```scala
import org.apache.spark.sql.functions.date_format
val preppedDataFrame = staticDataFrame
  .na.fill(0)
  .withColumn("day_of_week", date_format($"InvoiceDate", "EEEE"))
  .coalesce(5)
```

**Python**

```python
from pyspark.sql.functions import date_format, col
preppedDataFrame = staticDataFrame\
  .na.fill(0)\
  .withColumn("day_of_week", date_format(col("InvoiceDate"), "EEEE"))\
  .coalesce(5)
```

è¿˜éœ€è¦å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œå°†åœ¨æŸä¸ªè´­ä¹°å‘ç”Ÿçš„æ—¥æœŸä¹‹å‰æ‰‹åŠ¨æ‰§è¡Œæ­¤æ“ä½œï¼›ä½†æ˜¯ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨MLlibçš„ transformation apié€šè¿‡TrainValidationSplitæˆ–CrossValidatoråˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆå°†åœ¨ç¬¬å…­éƒ¨åˆ†è¯¦ç»†ä»‹ç»ï¼‰ï¼š

**Scala**

```scala
val trainDataFrame = preppedDataFrame.where("InvoiceDate < '2011-07-01'")
val testDataFrame = preppedDataFrame.where("InvoiceDate >= '2011-07-01'")
```

**Python**

```python
trainDataFrame = preppedDataFrame.where("InvoiceDate < '2011-07-01'")
testDataFrame = preppedDataFrame.where("InvoiceDate >= '2011-07-01'")
```

K-Meansèšç±»çš„ä»£ç ç¤ºä¾‹ï¼š

**Scala**

```scala
import org.apache.spark.ml.feature.OneHotEncoder
import org.apache.spark.ml.feature.StringIndexer
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.clustering.KMeans


val encoder = new OneHotEncoder().setInputCol("day_of_week_index")
  .setOutputCol("day_of_week_encoded")

val vectorAssembler = new VectorAssembler()
  .setInputCols(Array("UnitPrice", "Quantity", "day_of_week_encoded"))
  .setOutputCol("features")

val transformationPipeline = new Pipeline()
  .setStages(Array(indexer, encoder, vectorAssembler))

val fittedPipeline = transformationPipeline.fit(trainDataFrame)

val transformedTraining = fittedPipeline.transform(trainDataFrame)

transformedTraining.cache()

val kmeans = new KMeans()
  .setK(20)
  .setSeed(1L)

val kmModel = kmeans.fit(transformedTraining)
kmModel.computeCost(transformedTraining)
val transformedTest = fittedPipeline.transform(testDataFrame)
kmModel.computeCost(transformedTest)
```

**Python**

```python
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.clustering import KMeans


indexer = StringIndexer().setInputCol("day_of_week").setOutputCol("day_of_week_index")

encoder = OneHotEncoder().setInputCol("day_of_week_index")
  .setOutputCol("day_of_week_encoded")

vectorAssembler = VectorAssembler().setInputCols(["UnitPrice", "Quantity", 		        "day_of_week_encoded"]).setOutputCol("features")

fittedPipeline = transformationPipeline.fit(trainDataFrame)
transformedTraining = fittedPipeline.transform(trainDataFrame)
kmModel = kmeans.fit(transformedTraining)
kmModel.computeCost(transformedTraining)
transformedTest = fittedPipeline.transform(testDataFrame)
kmModel.computeCost(transformedTest)
```

## 3.4 å¼¹æ€§æ•°æ®é›†(RDD)ï¼šSparkçš„ä½çº§API

SparkåŒ…å«è®¸å¤šä½çº§APIï¼Œå…è®¸é€šè¿‡å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(RDD)å¯¹ä»»æ„Javaå’ŒPythonå¯¹è±¡è¿›è¡Œæ“ä½œã€‚å®é™…ä¸Šï¼ŒSparkä¸­çš„æ‰€æœ‰å†…å®¹éƒ½æ„å»ºåœ¨RDDä¹‹ä¸Šã€‚DataFrameæ“ä½œæ„å»ºåœ¨RDDä¹‹ä¸Šï¼Œå¹¶å‘ä¸‹ç¼–è¯‘åˆ°è¿™äº›è¾ƒä½çº§åˆ«çš„å·¥å…·ï¼Œä»¥æ–¹ä¾¿å’Œéå¸¸é«˜æ•ˆçš„åˆ†å¸ƒå¼æ‰§è¡Œã€‚æœ‰æ—¶å¯èƒ½ä¼šä½¿ç”¨RDDï¼Œç‰¹åˆ«æ˜¯åœ¨è¯»å–æˆ–æ“ä½œåŸå§‹æ•°æ®æ—¶ï¼Œä½†æ˜¯åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œåº”è¯¥åšæŒä½¿ç”¨ç»“æ„åŒ–APIã€‚RDDçš„çº§åˆ«ä½äºæ•°æ®å¸§ï¼Œå› ä¸ºå®ƒä»¬å‘æœ€ç»ˆç”¨æˆ·æš´éœ²ç‰©ç†æ‰§è¡Œç‰¹æ€§(å¦‚åˆ†åŒº)ã€‚

å¯èƒ½ä¼šä½¿ç”¨RDDæ¥å¹¶è¡ŒåŒ–å·²ç»å­˜å‚¨åœ¨é©±åŠ¨å™¨æœºå™¨å†…å­˜ä¸­çš„åŸå§‹æ•°æ®ã€‚ä¾‹å¦‚ï¼šå¹¶è¡ŒåŒ–ä¸€äº›ç®€å•æ•°å­—å¹¶åˆ›å»ºä¸€ä¸ªDataFrameã€‚å¯ä»¥å°†RDDè½¬æ¢ä¸ºDataFrame

RDDåœ¨Scalaå’ŒPythonä¸­éƒ½æœ‰ã€‚ç”¨æˆ·ä¸åº”è¯¥ä¸ºäº†æ‰§è¡Œè®¸å¤šä»»åŠ¡è€Œå¤§é‡ä½¿ç”¨RDDï¼Œé™¤éç»´æŠ¤çš„æ˜¯æ—§çš„Sparkä»£ç ã€‚æ–°ç‰ˆSparkä¸­å¤„ç†ä¸€äº›éå¸¸åŸå§‹çš„æœªå¤„ç†å’Œéç»“æ„åŒ–æ•°æ®ä¹‹å¤–ï¼Œåº”è¯¥ä½¿ç”¨RDDè€Œä¸æ˜¯ç»“æ„åŒ–apiã€‚

## 3.5 Sparkçš„ç”Ÿæ€ç³»ç»Ÿå’Œè½¯ä»¶åŒ…

Sparkæœ€æ£’çš„éƒ¨åˆ†ä¹‹ä¸€æ˜¯ç¤¾åŒºåˆ›å»ºçš„åŒ…å’Œå·¥å…·ç”Ÿæ€ç³»ç»Ÿã€‚éšç€è¿™äº›å·¥å…·çš„æˆç†Ÿå’Œå¹¿æ³›åº”ç”¨ï¼Œå…¶ä¸­ä¸€äº›å·¥å…·ç”šè‡³è¿›å…¥äº†æ ¸å¿ƒSparké¡¹ç›®ã€‚æ•°é‡è¶…è¿‡300ä¸ªæˆ–æ›´å¤šçš„è½¯ä»¶åŒ…ç»å¸¸è¢«æ·»åŠ ã€‚å¯ä»¥åœ¨Spark-Packages.orgä¸Šæ‰¾åˆ°SparkåŒ…çš„æœ€å¤§ç´¢å¼•ï¼Œåœ¨é‚£é‡Œä»»ä½•ç”¨æˆ·éƒ½å¯ä»¥å‘å¸ƒåˆ°æ­¤åŒ…å­˜å‚¨åº“ã€‚è¿˜æœ‰è®¸å¤šå…¶ä»–çš„é¡¹ç›®å’ŒåŒ…å¯ä»¥æ‰¾åˆ°ï¼Œä¾‹å¦‚ï¼Œåœ¨GitHubä¸Šã€‚